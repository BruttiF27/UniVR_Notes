\section{Dipendenza e Indipendenza Lineare}
Partiamo subito con un concetto necessario per introdurre l'argomento della sezione; è possibile che gli insiemi di generatori abbiano a loro volta dei sottoinsiemi. Prendiamo infatti lo spazio dei valori reali $\mathbb{K} = \mathbb{R}$, lo spazio vettoriale $V = \mathbb{R}^2$ e il seguente insieme di generatori:
\begin{center}
    $C = \left\{\begin{pmatrix}
        0\\
        1
    \end{pmatrix}, \begin{pmatrix}
        1\\
        0
    \end{pmatrix}, \begin{pmatrix}
        2\\
        3
    \end{pmatrix}\right\}$
\end{center}
I coefficienti per generare i vettori $x,y$ non sono univocamente determinati, infatti puoi arrivare ad una soluzione con più strade, basta pensare a come sono definiti gli insiemi numerici: ogni numero può essere rappresentato come somma o sottrazione di altri.\par\quad
Se si trovano soluzioni più efficienti per arrivare al vettore richiesto, ci si trova di fronte ad un \textbf{sottoinsieme di generatori}, i cui elementi devono necessariamente essere capaci di ricreare i vettori dell'insieme non utilizzati. In tal merito diciamo che $C$ è \textbf{linearmente dipendente}, poiché un suo vettore può essere espresso come combinazione lineare degli altri. Facciamo ora una piccola osservazione:
\begin{prop}
    Se $\{v_1,...,v_n\}$ è un insieme di generatori di uno spazio vettoriale $V$ e se $v_n$ è combinazione lineare di $v_1,...,v_{n-1}$, allora quest'ultimo insieme è un insieme di generatori di $V$. Di conseguenza, ciò che viene applicato agli insiemi di generatori vale anche per i loro sottoinsiemi.
\end{prop}
Ora possiamo definire formalmente l'argomento della sezione:
\begin{definition}
    \textbf{Dipendenza e Indipendenza Lineare}\par
    Dati i vettori di uno spazio vettoriale, quindi $v_1,...,v_n \in V$, il loro insieme si dice linearmente \textbf{dipendente} se almeno uno di quei vettori è una combinazione lineare degli altri. Ne consegue inoltre che se un insieme non è linearmente dipendente, sarà linearmente \textbf{indipendente}.
\end{definition}
Per dimostrare che un insieme di vettori è linearmente indipendente si utilizzano tre teoremi, tutti perfettamente equivalenti l'uno con l'altro.
\begin{theorem}
    Siano i vettori $v_1,...,v_n \in \mathbb{V}$, allora:
    \begin{enumerate}
        \item L'insieme $\{v_1, ..., v_n\}$ è linearmente indipendente.
        \item Se $\sum_{i=1}^{n} \alpha_i v_i = \sum_{i=1}^{n} \beta_i v_i$, per $\alpha_1,...,\alpha_n$ e $\beta_1,...,\beta_n$, entrambi appartenenti allo spazio $\mathbb{K}$, allora $\alpha_i = \beta_i \forall 1\leq i\leq n$.
        \item Se i valori $\alpha_1, ..., \alpha_n \in \mathbb{K}$ sono tali per cui $\alpha_1v_1 + ... + \alpha_nv_n = 0_v$, allora tutte le alfa saranno uguali a 0.
    \end{enumerate}
    Se valgono tutte e tre queste condizioni, hai dimostrato un'indipendenza lineare.
\end{theorem}
\begin{proof}
    Essendo che sono equivalenti, è possibile dimostrare uno a partire dall'altro, ma esiste una strategia migliore per contrarre i passaggi. Pay attention.\par
    Prima dimostra che $2 \to 3$, poi che $\neg2 \to \neg1 \to \neg3$. Iniziamo.
    \begin{itemize}
        \item $Th(2) \to Th(3)$\par
        Se la somma di prodotti dello spazio vettoriale è uguale a $0$, allora ogni scalare $\alpha_1 = ... = \alpha_n = 0$. Dimostrato.
        \item $\neg Th(2) \to \neg Th(1)$\par
        Supponiamo esistano scalari $\alpha_1,...,\alpha_n$ e $\beta_1, ..., \beta_n$, entrambi nello spazio $\mathbb{K}$ tali che $(\exists j.1\leq j\leq n)$ per il quale: $[(\alpha_j \neq \beta_j) \land (\sum_{i=1}^{n} \alpha_i v_i = \sum_{i=1}^{n} \beta_i v_i)]$.\par
        Abbiamo quindi che $0_v = (\sum_{i=1}^{n} \alpha_i v_i - \sum_{i=1}^{n} \beta_i v_i)$, che per proprietà associativa è uguale a $[\sum_{i=1}^{n}(\alpha_i-\beta_i)v_i]$, da cui $(\alpha_j - \beta_j \neq 0)$.\par
        Possiamo quindi dire che $0_v = \left[\sum_{i=1}^{n}(\dfrac{\alpha_i-\beta_i}{\alpha_j-\beta_j})v_i\right]$.\par
        In tal caso, avremo un $v_j = \left[\sum_{i=1}^{j-1}(\dfrac{\beta_i-\alpha_i}{\alpha_j-\beta_j})v_i\right] + \left[\sum_{i=j+1}^{n}(\dfrac{\beta_i-\alpha_i}{\alpha_j-\beta_j})v_i\right]$.
        \item $\neg Th(1) \to \neg Th(3)$\par
        Raggiunte le nostre ipotesi, teniamo buona la $j$ supposta e aggiungiamo che:\par
        $(\exists \alpha_1, ..., \alpha_{j-1}, \alpha_{j+1}, ..., \alpha_n \in \mathbb{K})$ tali che $v_j = [(\sum_{i=1}^{j-1} \alpha_i v_i) + (\sum_{i=j+1}^{n} \alpha_i v_i)]$\par
        Controlliamo dunque $0_v = v_j - v_j = \alpha_1v_1 + ... + \alpha_jv_j + ... + \alpha_nv_n$.\par
        Proprio qui la moltiplicazione fra $\alpha_jv_j \neq 0$, perché $\alpha_j = -1$. Assurdo. $Th(3)$ non vale.
    \end{itemize}
    Non valgono le ipotesi false, di conseguenza è stato dimostrato che i teoremi sono equivalenti e utilizzabili per dimostrare indipendenza lineare.
\end{proof}
\begin{eg}
    Dimostrazione di indipendenza lineare\par
    Dato lo spazio degli scalari $\mathbb{K} = \mathbb{R}$ e lo spazio vettoriale $\mathbb{V} = \mathbb{R}^2$, dimostriamo che l'insieme seguente è linearmente indipendente:
    \begin{center}
        $\left\{\begin{pmatrix}
            0\\
            1
        \end{pmatrix}, \begin{pmatrix}
            3\\
            2
        \end{pmatrix}\right\}$
    \end{center}
    Moltiplichiamo il vettore $0_v$ ad ogni elemento dell'insieme, il quale farà le veci per ogni $\alpha \in \mathbb{R}$. Ogni soluzione trovata sarà per forza uguale a 0. Per verificarlo moltiplica l'alfa corrispondente al vettore, noterai che risulterà sempre $0$. Per questo teorema, l'insieme è linearmente indipendente.
\end{eg}
L'esempio appena visto è applicabile anche alle matrici, pure con coefficienti complessi. Basterà semplicemente sostituire ad ogni alfa una matrice i cui elementi sono tutti uguali a $0$. Inoltre, generalmente diciamo che:
\begin{itemize}
    \item Un insieme $\{v\} \subseteq V$ è linearmente indipendente se s.se $v \neq 0_v$.
    \item Un insieme $\{v_1,v_2\} \subseteq V$ è linearmente dipendente s.se $\exists \alpha \in \mathbb{K}.v_1 = \alpha v_2$ o viceversa.
\end{itemize}
Andiamo avanti, perché ora possiamo parlare di \textbf{Basi} di uno spazio vettoriale. Diamone subito una definizione formale.
\begin{definition}
    \textbf{Base di uno spazio vettoriale}\par
    Sia $\mathbb{V}$ uno spazio vettoriale e $v_1,...,v_n \in \mathbb{V}$ i suoi elementi. L'insieme $B = \{v_1,...,v_n\}$ è detto base di $\mathbb{V}$ se:
    \begin{itemize}
        \item B è un insieme di generatori.
        \item B è linearmente indipendente.
    \end{itemize}
    In merito, $B$ è base di $\mathbb{V}$ s.se ogni vettore di $\mathbb{V}$ può essere scritto in modo unico come combinazione lineare degli elementi di $B$. Possiamo di conseguenza vedere la base come \textbf{sistema di coordinate} di $\mathbb{V}$. Possiamo infatti scrivere:
    \begin{center}
        $v = (\alpha_1 v_1 + ... + \alpha_n v_n$), con $\alpha_1, ..., \alpha_n$ univocamente determinati.\par
        Notazione: $[v]_B = \begin{pmatrix}
            \alpha_1\\
            ...\\
            \alpha_n
        \end{pmatrix}$ - Le alfa sono univocamente determinate perché $B$ è una base.
    \end{center}
    Dato ora lo spazio $\mathbb{V} = \mathbb{K}^n$, diremo infine che l'insieme $\mathcal{E}_n$ seguente, è la \textbf{Base canonica} dell'insieme $\mathbb{K}^n$.
    \begin{center}
        $\mathcal{E}_n = \left\{e_1 = \begin{pmatrix}
            1\\
            0\\
            ...\\
            0
        \end{pmatrix}, e_2 = \begin{pmatrix}
            0\\
            1\\
            ...\\
            0
        \end{pmatrix}, ..., e_n = \begin{pmatrix}
            0\\
            0\\
            ...\\
            1
        \end{pmatrix}\right\}$
    \end{center}
\end{definition}
\begin{eg}
    Determinare se $C$ è una base dello spazio $\mathbb{V} = \mathbb{R}^2$\par
    Per prima cosa bisogna dimostrare che $C$ è un insieme di generatori.
    \begin{center}
        $C = \left\{\begin{pmatrix}
            0\\
            1
        \end{pmatrix}, \begin{pmatrix}
            3\\
            2
        \end{pmatrix}\right\} \implies \begin{cases}
            0+3 = v_1\\
            1+2 = v_2
        \end{cases}$
    \end{center}
    Noti che puoi ottenere $v_1$ moltiplicando per $\dfrac{v_1}{3}$ la seconda colonna, modificando in tal modo il sistema:
    \begin{center}
        $\begin{cases}
            0+\dfrac{3}{3}v_1 = v_1\\
            1+\dfrac{2}{3}v_1 = v_2
        \end{cases} \implies \begin{cases}
            v_1 = v_1\\
            1+\dfrac{2}{3}v_1 = v_2
        \end{cases}$
    \end{center}
    Ora bisogna pensare a come ottenere $v_2$ con l'altra colonna. Hai la fortuna di avere un $1$, quindi puoi direttamente moltiplicare per $v_2$, che è la variabile necessaria, ma anche per $-\dfrac{2}{3}v_1$, per rimuovere la zavorra. Quindi:
    \begin{center}
        $\begin{cases}
            v_1 = v_1\\
            v_2-\dfrac{2}{3}v_1 + \dfrac{2}{3}v_1 = v_2
        \end{cases} \implies \begin{cases}
            v_1 = v_1\\
            v_2 = v_2
        \end{cases}$ Confermato insieme di generatori.\par
        Operazioni eseguite: $(r_1)(v_2-\dfrac{2}{3}v_1) + (r_2)(\dfrac{1}{3}v_1)$.
    \end{center}
    Per il punto 3 del teorema 7.3, $C$ è una base di $\mathbb{V}$. Useremo infine la notazione apposita per scrivere la base di uno spazio:
    \begin{center}
        $[v]_C = \begin{pmatrix}
            v_2 - \dfrac{2}{3}v_1\\
            \dfrac{1}{3}v_1
        \end{pmatrix}$
    \end{center}
    E hai finito.
\end{eg}
\begin{prop}
    \textbf{Base di una matrice ridotta}\par
    Considera la seguente matrice ridotta $U$. Le sue colonne dominanti formano una base dell'insieme di tutte le colonne $C(U)$:
    \begin{center}
        $U = \begin{pmatrix}
            1 & 3 & 5 & 3 & 2\\
            0 & 0 & 1 & -2 & 1\\
            0 & 0 & 0 & 0 & 1\\
            0 & 0 & 0 & 0 & 0
        \end{pmatrix} \implies \left\{\begin{pmatrix}
            1\\
            0\\
            0\\
            0
        \end{pmatrix}, \begin{pmatrix}
            5\\
            1\\
            0\\
            0
        \end{pmatrix}, \begin{pmatrix}
            2\\
            1\\
            1\\
            0
        \end{pmatrix}\right\} = B$
    \end{center}
    Dimostrare indipendenza lineare è presto detto grazie al punto 3 del teorema 7.3. Per dimostrare che $B$ è un insieme di generatori, invece, bisogna notare che i vettori di $C(U)$ sono combinazioni lineari dei vettori in $B$. Altrimenti puoi dimostrare la cosa facendo appello alla proposizione 7.1. Hai finito.
\end{prop}
Un'altra informazione importante è che le \textit{colonne} non nulle di una matrice ridotta $U^T$, ovvero le \textit{righe} non nulle di $U$, formano una base di $C(U^T)$. Non è finita qui; avendo una base di uno spazio vettoriale possiamo aggiungere questo corollario:
\begin{corollary}
    Sia $B = \{v_1, ..., v_n\}$ una base dello spazio vettoriale $V$ su $\mathbb{K}$. Allora:
    \begin{itemize}
        \item Diremo che $B$ è un \textbf{insieme di generatori minimo}, ovvero che nessun sottoinsieme proprio di $B$ è un insieme di generatori.
        \item $B$ è \textbf{massimamente linearmente dipendente}, ovvero che nessun insieme di vettori che contiene propriamente $B$ è linearmente indipendente.
    \end{itemize}
\end{corollary}
Un altro teorema molto comodo da ricordare è come qualunque spazio vettoriale non vuoto e finitamente generato abbia necessariamente una base. Segue teorema e dimostrazione.
\begin{theorem}
    Sia $V$ uno spazio vettoriale finitamente generato. Se non è nullo, ovvero che $V \neq \{0_v\}$, allora ha una base.
\end{theorem}
\begin{proof}
    Sia $B_n = \{v_1, ..., v_n\}$ un insieme di generatori. Se è anche linearmente indipendente, sussistono le condizioni per dire che è di conseguenza una base.\par\quad
    In caso contrario, uno dei vettori dell'insieme deve essere combinazione lineare degli altri.
\end{proof}
Aggiungiamo un ulteriore attrezzo all'officina; il \textbf{Teorema di Steinitz}, per il quale eviteremo la dimostrazione.
\begin{theorem}
    \textbf{Teorema di Steinitz}\par
    Siano $V$ uno spazio vettoriale su $\mathbb{K}$ e:
    \begin{itemize}
        \item $G = \{v_1, ..., v_n\}$ un insieme di generatori di $V$.
        \item $L = \{u_1, ..., u_m\}$ un insieme linearmente indipendente.
    \end{itemize}
    Allora possiamo dire che $m \leq n$ ed esiste un insieme di generatori di $V$ formato da $L$ e da $n-m$ vettori di $G$.
\end{theorem}
\begin{corollary}
    \textbf{Ogni base di uno stesso spazio ha lo stesso numero di elementi}\par
    Siano due basi $B_1 =\{v_1, ..., v_n\}, B_2 = \{u_1, ..., u_m\}$. Se sono entrambe basi di uno spazio $V$ su $\mathbb{K}$, allora $m = n$.
\end{corollary}
\begin{proof}
    Procediamo con la dimostrazione di quanto enunciato utilizzando il Teorema di Steinitz. Abbiamo che:
    \begin{itemize}
        \item Se $G = B_1 \land L = B_2 \to m \leq n$.
        \item Se $G = B_2 \land L = B_1 \to n \leq m$.
    \end{itemize}
    Ambo le condizioni ritornano che $n,m$ sono minori o uguali all'altro. Per logica saranno necessariamente uguali.
\end{proof}
Procediamo con una nuova nozione partendo sempre da uno spazio vettoriale. Possono essere quantificati in qualche modo? In questo caso entra in gioco la \textbf{dimensione} di uno spazio.
\begin{definition}
    \textbf{Dimensione di uno spazio vettoriale}\par
    Sia $V$ uno spazio vettoriale finitamente generato su $\mathbb{K}$. Il numero di vettori che formano una base di $V$ è chiamato dimensione di $V$ e si scrive con la notazione $dim_{\mathbb{K}}V$.
\end{definition}
Non è un concetto difficile da immaginare, basti pensare per esempio alla base canonica dello spazio $\mathbb{K}^n$, che indovina un pò, è uguale a $n$. La dimensione infatti dipende da $\mathbb{K}$. Un'ulteriore nozione comoda, partendo da uno spazio vettoriale $V$ su $\mathbb{K}$ finitamente generato, è che ogni insieme linearmente indipendente può essere completato ad una sua base. Infatti:
\begin{proof}
    \textbf{Completamento ad una base di insieme linearmente indipendente}\par
    Siano gli insiemi:
    \begin{itemize}
        \item $G = \{v_1, ..., v_n\}$ una base di $V$.
        \item $L = \{u_1, ..., u_m\}$ un insieme linearmente indipendente.
    \end{itemize}
    Per il teorema di Steinitz, abbiamo che esiste un insieme di generatori $B = L \cup G'$, dove l'insieme $G' \subseteq G$ ed il cui numero di elementi è dato da $n-m$.\par\quad
    Noti che $B$ è una base. Difatti, se fosse linearmente dipendente, conterrebbe una base di $V$ formata da meno di $n$ vettori.
\end{proof}
\begin{corollary}
    A ciò possiamo aggiungere i seguenti enunciati, partendo dal solito spazio di dimensione $n$:
    \begin{itemize}
        \item Un insieme con più di $n$ vettori è linearmente dipendente.
        \item Se $n$ vettori sono linearmente indipendenti, allora formano una base.
        \item Ogni insieme di generatori consiste di almeno $n$ vettori.
    \end{itemize}
\end{corollary}
Se ciò funziona per gli spazi, sicuramente andrà bene per i sottospazi. Prendiamo infatti uno spazio $V$, dove $dim_{\mathbb{K}}V = n$. Allora ogni sottospazio $U \subseteq V$ ha come dimensione $dim_{\mathbb{K}}U \leq n$. Inoltre, se il sottospazio è uguale allo spazio, avremo che la dimensione del primo è uguale ad $n$. Quindi che:
\begin{center}
    $dim_{\mathbb{K}}U = n \iff U = V$
\end{center}

%

\section{Applicazioni Lineari}
Iniziamo ora a parlare di vere e proprie applicazioni lineari. Per evitare confusione a causa della scuola superiore, questo argomento riguarda le \textbf{funzioni}, ma noi siamo bravih e utilizzeremo i termini appropriati. Inoltre, da ora, ogni spazio vettoriale menzionato sarà finitamente generato.\par\quad
Ma bando alle ciance, iniziamo col dare la definizione di applicazione lineare.
\begin{definition}
    \textbf{Applicazione lineare}\par
    Siano $U,V$ due spazi vettoriali su $\mathbb{K}$. Un'applicazione $f:U \to V$ si dice \textbf{lineare} se per ogni $u,u' \in U$ e ogni $\alpha \in \mathbb{K}$ valgono i seguenti enunciati:
    \begin{itemize}
        \item $f(u+u') = f(u) + f(u')$.
        \item $f(\alpha u) = \alpha f(u)$.
    \end{itemize}
    \begin{prop}
        In merito, sia un'applicazione lineare $f:U\to V$. Valgono le seguenti relazioni:
        \begin{itemize}
            \item $f(0_v) = f(0 \times 0_v) = 0\times f(0_v) = 0_v$.
            \item Se $u \in U \to -u \in U$\par
            $f(-u) = f(-1\times u) = -1\times f(u) = -f(u)$
        \end{itemize}
    \end{prop}
\end{definition}
Proviamo adesso a vedere un esempio di come funzionino le applicazioni lineari e le modalità di dimostrazione. Tieni a mente che quanto si vedrà ora vale non solo per vettori e spazi vettoriali, bensì anche per le matrici, per le quali sarà lasciato un esercizio a fondo sezione.
\begin{eg}
    Siano l'applicazione $f:U \to V$ e gli spazi $U,V$ tali che:
    \begin{itemize}
        \item $U = \mathbb{R}_2[x] = \{p = a_0+a_1x+a_2x^2|a_0,a_1,a_2 \in \mathbb{R}\}$
        \item $V = \mathbb{R}^2$
    \end{itemize}
    Applichiamo il polinomio di $U$ alla funzione, la quale ritornerà un risultato in $V$. Effettuiamo quindi l'operazione:
    \begin{center}
        $p \to f(p) = \begin{bmatrix}
            p(0)\\
            p(1)
        \end{bmatrix} \in \mathbb{R}^2$
    \end{center}
    Fondamentalmente bisognerà sostituire il numero nelle parentesi di $p$ al posto delle $x$. Mettiamo che il polinomio di $U$ sia $p = -x^2+3x+1$. Allora:
    \begin{center}
        $f(p) = \begin{bmatrix}
            p(0)\\
            p(1)
        \end{bmatrix} = \begin{bmatrix}
            -0^2+3\times 0+1\\
            -1^2+3\times 1+1
        \end{bmatrix} = \begin{bmatrix}
            1\\
            5
        \end{bmatrix}$
    \end{center}
    Per confermare che l'applicazione sia lineare ora bisogna verificare che reggano le due proprietà viste prima. Siano due polinomi: $p = a_0+a_1x+a_2x^2$, $q = b_0+b_1x+b_2x^2$ entrambi sullo spazio $U$.
    \begin{itemize}
        \item $f(p+q) = f(p) + f(q)$:\par
        $f(p+q) = \begin{bmatrix}
            a_0+b_0\\
            a_0+b_0+a_1+b_1+a_2+b_2
        \end{bmatrix}$\par
        $f(p)+f(q) = \begin{bmatrix}
            a_0\\
            a_0+a_1+a_2
        \end{bmatrix}+\begin{bmatrix}
            b_0\\
            b_0+b_1+b_2
        \end{bmatrix} = \begin{bmatrix}
            a_0+b_0\\
            a_0+b_0+a_1+b_1+a_2+b_2
        \end{bmatrix}$ Ok.
        \item $f(\alpha p) = \alpha f(p)$:\par
        $f(\alpha p) = \begin{bmatrix}
            \alpha a_0\\
            \alpha a_0 + \alpha a_1 + \alpha a_2
        \end{bmatrix} = \begin{bmatrix}
            \alpha a_0\\
            \alpha(a_0+a_1+a_2)
        \end{bmatrix}$\par
        $\alpha f(p) = \alpha \begin{bmatrix}
            a_0\\
            a_0+a_1+a_2
        \end{bmatrix} = \begin{bmatrix}
            \alpha a_0\\
            \alpha a_0 + \alpha a_1 + \alpha a_2
        \end{bmatrix} = \begin{bmatrix}
            \alpha a_0\\
            \alpha(a_0+a_1+a_2)
        \end{bmatrix}$ Also ok.
    \end{itemize}
    Abbiamo dimostrato che l'applicazione $f:U\to V$ è lineare perché valgono le due relazioni supposte.
\end{eg}
Inoltre, generalmente, per ogni applicazione lineare del tipo $f:\mathbb{K}^n \to \mathbb{K}^m$ e per ogni vettore $v \in \mathbb{K}^n$ abbiamo le seguenti relazioni equivalenti:
\begin{itemize}
    \item $f(v) = f(v_1e_1 + v_2e_2 + ... + v_ne_n)$
    \item $f(v) = v_1f(e_1) + v_2f(e_2) + ... + v_nf(e_n)$
    \item $f(v) = (f(e_1), f(e_2), ..., f(e_n)) \times (v_1, ..., v_n)$
\end{itemize}
Ovvero che $f = f_A$, dove $A = (f(e_1), f(e_2), ..., f(e_n)) \in M_{m\times n}(\mathbb{K})$. Questa matrice si chiama \textbf{associata ad f rispetto alla base canonica}.\par\quad
Poi, sia $B$ una base di $V$ su $\mathbb{K}$. Ogni vettore $v$ può essere scritto in modo unico con la seguente formula:
\begin{center}
    $v = \alpha_1b_1 + ... + \alpha_nb_n$, dove $\alpha_1, ..., \alpha_n \in \mathbb{K}$.
\end{center}
Una scrittura unica? Inutile girarci intorno; possiamo usarla come sistema di coordinate.
\begin{definition}
    \textbf{Applicazione delle coordinate rispetto a una base}\par
    Definiamo $[v]_B$. L'applicazione $c_B:V \to \mathbb{K}^n$ è lineare.
    \begin{center}
        $[v]_B = \begin{pmatrix}
            \alpha_1\\
            ...\\
            \alpha_n
        \end{pmatrix} = c_B(v)$
    \end{center}
    Per dimostrarlo, è sufficiente verificare le due relazioni di prima. Fidati che risultano vere. Grazie a ciò chiamiamo l'applicazione lineare $c_B$ l'\textbf{applicazione delle coordinate rispetto alla base ordinata B}.
\end{definition}
\begin{eg}
    Sia lo spazio $V = \mathbb{R}_2[x]$ e la base $B = \{b_1 = 1+x, b_2 = 1+x^2, b_3 = x+x^2\}$\par
    Prendiamo il vettore utile $v = 3+2x-x^2 \in V$, che sarà utilizzato poi come vettore soluzioni, e calcoliamo lo spazio delle colonne $c_B(v)$, che sarà la base, quindi è $[v]_B$.\par
    Capiamo che $B$ è una base di $v$, sappiamo che esistono tre scalari $\alpha_1, \alpha_2, \alpha_3 \in \mathbb{R}$ tali che:
    \begin{center}
        $v = \alpha_1b_1 + \alpha_2b_2 + \alpha_3b_3$
    \end{center}
    Ovvero che, sostituendo alle $b$ i loro rispettivi valori otteniamo:
    \begin{center}
        $v = \alpha_1(1+x) + \alpha_2(1+x^2) + \alpha_3(x+x^2) = (\alpha_1+\alpha_2)+(\alpha_1+\alpha_3)x+(\alpha_2+\alpha_3)x^2$
    \end{center}
    Abbiamo quindi gli scalari e le soluzioni. Cosa possiamo creare? Che domande, ma una matrice aumentata, ovviamente. Da qua puoi ridurla al minimo con Gauss come al solito.
    \begin{center}
        $\begin{cases}
            \alpha_1+\alpha_2 = 3\\
            \alpha_1+\alpha_3 = 2\\
            \alpha_2+\alpha_3 = -1
        \end{cases} \implies \begin{pmatrix}
            1 & 1 & 0 & 3\\
            1 & 0 & 1 & 2\\
            0 & 1 & 1 & -1
        \end{pmatrix} \implies \begin{pmatrix}
            1 & 0 & 0 & 3\\
            0 & 1 & 0 & 0\\
            0 & 0 & 1 & -1
        \end{pmatrix}$
    \end{center}
    Adesso possiamo risolvere il sistema lineare preso dalla matrice aumentata ridotta.
    \begin{center}
        $\begin{cases}
            \alpha_1 = 3\\
            \alpha_2 = 0\\
            \alpha_3 = -1
        \end{cases} \implies v = 3b_1 + 0b_2 - b_3 \implies [v]_B = \begin{pmatrix}
            3\\
            0\\
            1
        \end{pmatrix}$
    \end{center}
\end{eg}
Un'altra cosa utile da tenere a mente è che generalmente, se una base $B = \{b_1, ..., b_n\}$ è una base di $\mathbb{K}^n$, allora la matrice $A = (b_1, ..., b_n)$ è invertibile e $c_B = f_{A^-1}$.\par\quad
Passiamo al prossimo macroargomento: gli \textbf{isomorfismi}; i quali ci consentiranno di espandere le nostre competenze usando funzioni inverse.
\begin{definition}
    \textbf{Isomorfismo}\par
    Sia un'applicazione lineare $f:V \to W$. Questa è detta \textbf{isomorfismo} se esiste un'altra applicazione lineare $g:W \to V$ tale che:
    \begin{itemize}
        \item $gf(v) = v$ per ogni $v \in V$.
        \item $fg(w) = w$ per ogni $w \in W$.
    \end{itemize}
    Diremo quindi che $g$ è l'inversa di $f$ e la denoteremo come $f^{-1}$. Inoltre gli spazi $W, V$ sono isomorfi e vengono indicati con $V \cong W$.
    \begin{prop}
        \textbf{Egual dimensione}\par
        Siano ora $V, W \in \mathbb{K}$ due spazi vettoriali e $f:V\to W$ un isomorfismo.\par\quad
        Se $B = \{b_1, ..., b_n\}$ è una base di $V$, allora $C = \{f(b_1), ..., f(b_n)\}$ è una base di $W$, ed in particolare $dim_{\mathbb{K}}V = dim_{\mathbb{K}}W = n$. Non a caso valgono queste due relaizoni:
        \begin{itemize}
            \item $C$ è un insieme di generatori di $W$.\par
            Infatti per ogni $w \in W$ esistono $n$ scalari in $\mathbb{K}$ tali che $f^{-1} \in V$, $f^{-1} = \alpha_1b_1+...+\alpha_nb_n$, da cui:
            \begin{center}
                $w = ff^{-1}(w) = f(\alpha_1b_1+...+\alpha_nb_n) = \alpha_1f(b_1) + ... + \alpha_nf(b_n)$.
            \end{center}
            \item $C$ è linearmente indipendente.\par
            Considera, se $0_w = \alpha_1f(b_1) + ... + \alpha_nf(b_n)$, proviamo a trovare $0_v = f^{-1}(0_w)$.
            \begin{center}
                $f^{-1}(0_w) = f^{-1}(\alpha_1f(b_1) + ... + \alpha_nf(b_n)$, raccogliendo la $f$...\par
                $f^{-1}f(\alpha_1b_1+...+\alpha_nb_n)$
            \end{center}
            Ora, siccome $B$ è una base di $V$, abbiamo che tutti gli scalari sono uguali a $0$, quindi $\alpha_1, ..., \alpha_n = 0$, confermando l'indipendenza lineare.
        \end{itemize}
    \end{prop}
    \begin{prop}
        \textbf{Matrici associate alle applicazioni}\par
        Diciamo ora di avere un isomorfismo $f:\mathbb{K}^n \to \mathbb{K}^m$ e $A$ la matrice ad esso associata, quindi che $f(v) = f_A(v) = Av$, dove $v \in \mathbb{K}^n$.\par\quad
        Consideriamo ora la funzione inversa di $f$, che esiste necessariamente in quanto isomorfismo, e la matrice $B$ ad essa associata, quindi $f^{-1}(w) = f_B(w) = Bw$, dove $w \in \mathbb{K}^m$. Tutto ciò porta ad avere:
        \begin{itemize}
            \item Per ogni $v \in \mathbb{K}^n$: $v = f^{-1}f(v) = f^{-1}Av = BAv \implies BA = I_n$.
            \item Per ogni $w \in \mathbb{K}^m$: $w = ff^{-1}(w) = fBw = ABw \implies AB = I_m$.
        \end{itemize}
        Possiamo concludere che $f_A$ è un isomorfismo poiché abbiamo capito che $A$ è una matrice invertibile, \textbf{condizione necessaria e sufficiente} per il fatto. Proviamolo:
        \begin{itemize}
            \item Per ogni $v \in \mathbb{K}^n$: $f_{A^{-1}}f_A(v) = A^{-1}Av = v$.
            \item Per ogni $w \in \mathbb{K}^m$: $f_Af_{A^{-1}}(w) = AA^{-1}w = w$. 
        \end{itemize}
    \end{prop}
\end{definition}
Abbiamo parlato di scritture univocamente determinate, no? Possiamo passare finalmente all'applicazione delle coordinate sugli spazi. Diamone inizialmente una definizione.
\begin{theorem}
    \textbf{Applicazione di coordinate è un isomorfismo}\par
    Sia $V$ uno spazio vettoriale su $\mathbb{K}$ e $B = \{b_1, ..., b_n\}$ una sua base. L'applicazione delle coordinate seguente è un isomorfismo:
    \begin{center}
        $c_B:V \to \mathbb{K}^n$.
    \end{center}
    \begin{proof}
        Come dimostrare un isomorfismo? Trovare un'applicazione lineare inversa di quella fra le nostre mani. Quindi che esista:
        \begin{center}
            $g_B:\mathbb{K}^n \to V$, dove $\mathbb{K}^n = \begin{pmatrix}
                \alpha_1\\
                ...\\
                \alpha_n
            \end{pmatrix}$, $V = g_B(\sum_{i=1}^{n} \alpha_i\beta_i)$
        \end{center}
        Procediamo, dunque, a dimostrare che $g_B$ è l'inversa di $c_B$:
        \begin{center}
            $c_Bg_B \begin{pmatrix}
                \alpha_1\\
                ...\\
                \alpha_n
            \end{pmatrix} = c_B(\sum_{i=1}^{n} \alpha_i\beta_i) = [\sum_{i=1}^{n} \alpha_i\beta_i]_B = \begin{pmatrix}
                \alpha_1\\
                ...\\
                \alpha_n
            \end{pmatrix}$
        \end{center}
        Sia ora $v = \sum_{i=1}^{n} \alpha_i\beta_i \in V$, allora:
        \begin{center}
            $g_Bc_B(v) = g_B([\sum_{i=1}^{n} \alpha_i\beta_i]) = g_B\begin{pmatrix}
                \alpha_1\\
                ...\\
                \alpha_n
            \end{pmatrix} = \sum_{i=1}^{n} \alpha_i\beta_i = v$.
        \end{center}
        Abbiamo ottenuto $v$, CVD, $c_B$ è un isoformismo e $g_B$ la sua inversa.
    \end{proof}
    \begin{corollary}
        \textbf{Isomorfismo e dimensione}\par
        Diciamo che due spazi vettoriali $V, W$ sono isomorfi se e solo se hanno egual dimensione, quindi $dimV = dimW$.
        \begin{proof}
            Supponiamo esista un isomorfismo $f:V\to W$, $B$ base di $V$ e $D$ base di $W$. Possiamo usare l'isomorfismo $c_B$ e ottenere la scrittura:
            \begin{center}
                $c_Dfg_B = c_Dfc_B^{-1}:\mathbb{K}^n \to \mathbb{K}^m$
            \end{center}
            C'è roba da aggiungere qua.
        \end{proof}
    \end{corollary}
\end{theorem}





7.9 Teorema e definizione: matrice del cambio di base
7.10 Teorema e definizione: matrice associata a f rispetto a basi

% ----- CONTINUA DA QUA LEZ. 25 pag. 5 -----

\section{Rango e Nullità}
§8. Rango e nullit`a (vedi [GS, Capitolo II])
8.1 Spazio nullo e immagine di un’applicazione lineare
22/04/24
8.2 Teorema: nullit`a + rango
8.3 Dimensione di C(A)
8.4 Dimensione di N (A)
8.5 Procedimento per determinare basi di C(A) e N (A)
8.6 Proposizione e definizione: rango di un’applicazione lineare
8.7 Teorema: insieme di soluzioni di un sistema lineare

%

\section{Esercizi}

% Inserisci esercizio alla fine di lezione 22 qui in basso