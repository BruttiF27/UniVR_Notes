\section{Elementi di probabilità}
La probabilità è una branca della matematica che si occupa dello studio e descrizione degli \textbf{esperimenti aleatori}, ovvero delle inferenze il cui esito non è del tutto prevedibile. Esistono due metodi per l'espressione del concetto di probabilità:
\begin{itemize}
	\item \textbf{Approccio frequentista}; Determinazione della probabilità mediante esperimenti ripetuti. Risulta quindi come il rapporto fra il totale in cui si è esperito un esito e il totale degli esperimenti.
	\item \textbf{Approccio soggettivista}; Dove la probabilità è vista come un livello di fiducia nel verificarsi di un dato esito. È na roba da filosofi, non fa per noi.
\end{itemize}
Abbiamo parlato di una totalità di esperimenti; questi vengono formalmente chiamati \textbf{eventi} $E$ e detengono informazioni riguardo al loro esito. Ogni evento è un sottoinsieme dello \textbf{spazio campionario} $S$, che li comprende tutti.
\begin{eg}
	\textbf{Spazio campionario}\newline
	Un esempio di spazio campionario è dato dalla totalità dei valori delle facce di un dado, mentre gli eventi sono i singoli valori usciti da un esperimento.
	\begin{center}
		$S = \{1, 2, 3, 4, 5, 6\}$, $E = \{4\}$
	\end{center}
\end{eg}
Alle operazioni logiche sulle affermazioni corrispondono quelle insiemistiche,le quali si mostrano mediante diagrammi di eulero venn. Siano $A, B \subseteqq S$ due eventi:
\begin{itemize}
	\item \textbf{Intersezione}\newline
	Quando "Avviene $A$ e avviene $B$"
	% TODO ----- AGGIUNGI DIAGRAMMA
	\item \textbf{Unione}\newline
	Quando "Avviene $A$ oppure $B$"
	% TODO ----- AGGIUNGI DIAGRAMMA
	\item \textbf{Sottrazione}\newline
	Quando "Avviene $A$, ma non $B$"
	% TODO ----- AGGIUNGI DIAGRAMMA
	\item \textbf{Complementare}\newline
	Quando "Non avviene $A$"
\end{itemize}
Inoltre, se gli insiemi $A,B$ sono tali che la loro intersezione sia vuota, si dicono \textbf{incompatibili}.

%

\section{Calcolo della probabilità}
Fortunatamente esiste una concezione standard sulle caratteristiche assunte dalla probabilità. Associamo infatti ad ogni evento $E$ sullo spazio campionario $S$, un valore denotato con $P(E)$, detto \textbf{probabilità dell'evento} $E$. Il comportamento della funzione è dato dai seguenti \textbf{assiomi di Kolmogorov}:
\begin{definition}
	\textbf{Assiomi di Kolmogorov}\newline
	\begin{enumerate}
		\item $P(A)$ è un valore compreso fra $0$ e $1$.
		\item $P(S) = 1$.
		\item Se $A$ e $B$ sono incompatibili, allora $P(A \cup B) = P(A) + P(B)$.
		\item Siano $A, B$ due eventi tali che $A \subseteqq B$, allora:
		\begin{center}
			$\begin{cases}
				B = S \implies S / A = A^c \implies P(A^c) = 1 - P(A)\\
				P(B / A) = P(B) - P(A)
			\end{cases}$
		\end{center}
		\item Se $A_1, A_2, ..., A_k$ sono eventi a due a due incompatibili, quindi disgiunti, allora:
		\begin{center}
			\[P\left(\bigcup_{i = 1}^k A_i\right) = \sum_{i = 1}^{k} P(A_i)\]
		\end{center}
		\item Siano $A, B$ due eventi generici, allora:
		\begin{center}
			$P(A \cup B) = P(A) + P(B) - P(A \cap B)$
		\end{center}
	\end{enumerate}
\end{definition}
Un primo caso di studio per la probabilità è il suo calcolo ad \textbf{esiti equiprobabili}; ciò significa che ogni evento ha la stessa chance di avvenire rispetto agli altri. L'esempio classico è il lancio di un dado; implicando che questo non sia truccato, ogni faccia ha $\frac{1}{6}$ di possibilità di uscire. Formalmente la definiamo con la seguente scrittura:
\begin{center}
	$P(A) = \dfrac{|A|}{|S|}$
\end{center}
\begin{eg}
	Quali sono le probabilità che lanciando due volte un dado esca il valore 7?\newline
	Innanzitutto dobbiamo chiederci quale sia lo spazio campionario e gli eventi. Sappiamo che è un dado, quindi avremo rispettivamente:
	\begin{itemize}
		\item $S = \{1, 2, 3, 4, 5, 6\}$
		\item $E_1 = \{1\}, ..., E_6 = \{6\}$
	\end{itemize}
	Ora, potremmo fare bruteforcing facendoci del male, ma il trucco per questi esercizi (entro certi limiti) è disegnare una tabella dei risultati, prendere il totale di quante volte si presenta il valore richiesto e poi applicare la formula dell'approccio frequentista. In questo caso:
	\begin{center}
		\begin{tabular}{|c||c|c|c|c|c|c|}
			\hline
			- & 1 & 2 & 3 & 4 & 5 & 6\\
			\hline
			\hline
			1 & 2 & 3 & 4 & 5 & 6 & 7\\
			\hline
			2 & 3 & 4 & 5 & 6 & 7 & 8\\
			\hline
			3 & 4 & 5 & 6 & 7 & 8 & 9\\
			\hline
			4 & 5 & 6 & 7 & 8 & 9 & 10\\
			\hline
			5 & 6 & 7 & 8 & 9 & 10 & 11\\
			\hline
			6 & 7 & 8 & 9 & 10 & 11 & 12\\
			\hline
		\end{tabular}
	\end{center}
	Notiamo che il valore $7$ compare $6$ volte ed il totale degli esiti ottenibili è $6 \times 6 = 36$. Il risultato sarà dato quindi da:
	\begin{center}
		$\dfrac{6}{36} = \dfrac{1}{6}$, soluzione dell'esercizio.
	\end{center}
\end{eg}
Puta caso, devi lavorare con una quantità di dati abnorme; utilizzare la tabella precedente per analizzare è impensabile, hai una vaga idea di quanto grande verrebbe? Viene ad aiutarci il seguente ragionamento, scrivibile tramite \textbf{coefficienti binomiali}:
\begin{eg}
	\textbf{Calcolo combinatorio con coefficiente binomiale}\newline
	Diciamo di avere una gara a cui partecipano 10 atleti. In quanti modi possiamo assegnare i vari posti del podio? Avremo:
	\begin{itemize}
		\item 10 modi per il primo posto.
		\item 9 modi per il secondo, in quanto il primo è già stato assegnato.
		\item 8 modi per il terzo, per la medesima ragione.
	\end{itemize}
	Supponiamo di voler uccidere tutti quelli che perdono e che quindi considereremo solo i tre posti del podio. Allora quali sarebbero i possibili esiti?
	\begin{center}
		ABC, ACB, CAB, CBA, BAC, BCA, quindi $3\times 2\times 1 = 6$ esiti.
	\end{center}
	Questo calcolo si può esprimere più facilmente mediante l'utilizzo dei fattoriali, Gli esiti totali possibili saranno quindi:
	\begin{center}
		$\dfrac{(10\times9\times8)}{(3\times2\times1)} = \dfrac{10!}{7!\times3!}$
	\end{center}
	Perché è sbucato fuori un $7!$ dal nulla? Ebbene, quelli sono tutti i numeri che non ci interessano, in quanto vogliamo solamente i posti del podio.
\end{eg}
\noindent Da questo esempio traiamo dunque una formula generale, in inglese chiamata \textbf{n choose k}, la quale ha due varianti dipendentemente se ci interessa (caso 1) o meno (caso 2) l'ordine dei dati:
\begin{center}
	$\begin{cases}
		\dfrac{n!}{(n-k)!k!}\\
		\dfrac{n!}{(n-k)!}
	\end{cases} \implies \begin{pmatrix}
	n\\
	k
	\end{pmatrix}$
\end{center}

% TODO ----- Aggiungi esempio del mazzo da poker

%

\section{Probabilità condizionata}
Finora abbiamo utilizzato l'approccio frequentista per il calcolo delle probabilità di un evento, considerandole come a loro stanti. È tuttavia possibile che la probabilità di un evento $A$ possa essere influenzata da un altro $B$. Chiamiamo questo concetto \textbf{probabilità condizionata} e prende la formula matematica:
\begin{center}
	$P(A|B) = \dfrac{P(A\cap B)}{P(B)}$
\end{center}
Personalmente leggo la formula come "probabilità di $A$ sotto $B$". Quest'ultimo evento può quindi influenzare il primo positivamente, aumentandone la probabilità, oppure negativamente, diminuendola. Un'altra particolarità riguarda lo spazio campionario; essendo che stiamo valutando un'istanza dove $B$ avviene sicuramente, sarà proprio questo lo spazio. Possiamo infatti spaccare le istanze:
\begin{itemize}
	\item Succedono $A$ e $B$; quindi la probabilità condizionata.
	\item Succede $A$, ma non $B$; quindi la probabilità senza influenze.
\end{itemize}
È necessario conoscere i valori di entrambe le istanze per il calcolo della probabilità effettiva, infatti compone la seguente:
\begin{definition}
	\textbf{Formula delle probabilità totali}\par
	Formula utilizzata per il calcolo delle probabilità di un evento il cui esperire è condizionato da un altro. Siano $A,B$ due eventi generici.
	\begin{center}
		$P(A) = (A|B)P(B) + P(A|B^c)P(B^c)$
	\end{center}
	Dove il primo addendo rappresenta la probabilità condizionata ed il secondo quella di $A$ a sé stante.
\end{definition}

\begin{eg}
	\textbf{Calcolo di probabilità condizionata}\newline
	\noindent Siano due urne tali che:
	\begin{itemize}
		\item A contiene 2 palline rosse e 4 verdi.
		\item B contiene 3 palline rosse e 2 verdi.
	\end{itemize}
	Si lancia ora un dado; se esce 6 si estrae da A, altrimenti da B. Calcolare la probabilità di estrarre una pallina verde.\par
	Introduciamo i seguenti due eventi in base al risultato del dado:
	\begin{enumerate}
		\item $E$, Il dado mostra $6$.
		\item $F$, La pallina estratta è verde.
	\end{enumerate}
	\noindent Potremmo elencare ogni singola permutazione dati i pochi casi, ma useremo la formula della probabilità totale per pulizia. Attualmente deteniamo i seguenti dati:
	\begin{itemize}
		\item $P(F|E) = \frac{4}{6}$, date le $6$ palline di A, di cui $4$ verdi.
		\item $P(F|E^c) = \frac{2}{5}$, date le $5$ palline di B, di cui $2$ verdi.
		\item $P(E) = \frac{1}{6}$, probabilità del dado di far uscire $6$.
		\item $P(E^c) = \frac{5}{6}$, ogni altro numero del dado.
	\end{itemize}
	Ciò che abbiamo è sufficiente per utilizzare la formula della probabilità totale. Risulterà infatti:
	\begin{center}
		$P(F) = P(F|E)P(E) + P(F|E^c)P(E^c) = \dfrac{4}{6}\times \dfrac{1}{6} + \dfrac{2}{5}\times \dfrac{5}{6} = \dfrac{4}{9}$
	\end{center}
\end{eg}
\noindent Se pensi che sia possibile ottenere algebricamente le altre probabilità della formula, hai avuto un'ottima idea. Infatti per le probabilità condizionate abbiamo un nome apposito, che è:
\begin{definition}
	\textbf{Formula e teorema di Bayes}\par
	Necessaria per il calcolo della singola probabilità condizionata di un evento.
	\begin{center}
		$P(A|B) = \dfrac{P(B|A)P(A)}{P(B)}$
	\end{center}
	Sia ora ${B_1, B_2, ..., B_n}$ una partizione dello spazio campionario. Ne segue il teorema:
	\begin{center}
		$P(B_i|A) = \dfrac{P(A|B_i)P(B_i)}{\sum^n_{j=1} P(A|B_j)P(B_j)}$
	\end{center}
\end{definition}

\begin{eg}
	\textbf{Calcolo di probabilità con formula di Bayes}\par
	\noindent Abbiamo un esame a 4 risposte multiple. Gli studenti iscritti si dividono in:
	\begin{itemize}
		\item Preparato, corrispondente all'80\% del totale. Risponde correttamente al 90\%.
		\item Impreparato, il 20\% rimanente, che risponde a caso. Quindi hanno un 25\% di azzeccare la risposta.
	\end{itemize}
	Qual è la probabilità di prendere l'esame di uno studente preparato fra tutti?\newline
	
	\noindent Definiamo gli eventi come $A$, ovvero che lo studente sia preparato, e $B$, quella di azzeccare una risposta, che è necessariamente condizionata dal primo evento. Elenchiamo i dati che abbiamo fin da subito:
	\begin{itemize}
		\item $P(A) = 0,8$
		\item $P(A^c) = 0,2$
		\item $P(B|A) = 0,9$
		\item $P(B|A^c) = 0,25$
	\end{itemize}
	Dobbiamo trovare $P(A|B)$, ma procediamo per passi. Innanzitutto ci serve $P(B)$, ovvero la probabilità di azzeccare la risposta in generale. Usiamo la formula delle probabilità totali:
	\begin{center}
		$P(B) = P(B|A)P(A) + P(B|A^c)P(A^c) = 0,9\times 0,8 + 0,25\times 0,2 = 0,72 + 0,05 = 0,77$
	\end{center}
	Ora possiamo muoverci facendo delle asserzioni algebriche. Considera che $P(A|B)P(B) = P(A\cap B) = P(B\cap a) = P(B|A)P(A)$, da cui otteniamo la formula di Bayes: 
	\begin{center}
		$P(A|B) = \dfrac{P(B|A)P(A)}{P(B)}$
	\end{center}
	La quale, sostituendo le variabili con i loro rispettivi valori, ci darà il risultato:
	\begin{center}
		$P(A|B) = \dfrac{0,72}{0,77} = 0.935$
	\end{center}	
\end{eg}
E se invece avessimo due eventi completamente \textbf{indipendenti}? Questi si dicono tali se, dati per esempio A, B, vale la relazione:
\begin{center}
	$P(A \cap B) = P(A)P(B)$
\end{center}
Per esempio, se lancio due dadi, la probabilità che escano i valori 6 e 5 separatamente è $\frac{1}{36}$, perché mi va bene una sola combinazione. Infatti:
\begin{center}
	$P(A \cap B) = \dfrac{1}{6}\times \dfrac{1}{6} = \dfrac{1}{36}$
\end{center}
Abbiamo appurato che i due dadi non si influenzano fra di loro. Se ci fossero invece tre eventi avremmo le seguenti relazioni, dati A, B, C:
\begin{itemize}
	\item $P(A \cap B \cap C) = P(A)P(B)P(C)$
	\item $P(A \cap B) = P(A)P(B)$
	\item $P(A \cap C) = P(A)P(C)$
	\item $P(B \cap C) = P(B)P(C)$
\end{itemize}
Ovviamente, per casi richiedenti più di tre eventi, si dovranno verificare le istanze per tutti i successivi.

%

\section{Variabili aleatorie}
Le variabili aleatorie sono quantità numeriche il cui valore dipende dall'esito di un esperimento aleatorio; si indicano con $X, Y, Z$. La loro primaria utilità sta nel consentirci di poter considerare un risultato specifico al posto di ogni singolo evento.\par 
Per esempio, diciamo di lanciare due dadi e voler sapere la somma dei valori dei numeri che escono. Allora la variabile $X$ potrà assumere un valore dell'evento $x$ tale che:
\begin{center}
	$[(x\in S) \land (2 \leq x \leq 12)]$, dove $S$ è lo spazio campionario.
\end{center}
\noindent Possiamo inoltre ragionarci con la probabilità. Diciamo di voler effettuare un numero $n$ di lanci e che ci interessi vedere le chances che esca un dato numero. In questo caso $X$ sarà il valore totale dei successi ottenuti e la probabilità sarà data da $P(X = x)$.\newline

\noindent Di variabili aleatorie ne esistono due tipi:
\begin{itemize}
	\item \textbf{Discrete}; Se i valori che può assumere sono finiti o al più numerabili; quindi in un insieme $S = \{x_1, x_2, ..., x_n, ...\}$. Da questo spazio sappiamo che il calcolo della sua probabilità si effettua con la \textbf{funzione di massa}:
	\begin{center}
		$p(x) = P(X = x)$, dove grazie all'algebra vale $p(x_1), p(x_2), ..., p(x_n)$
	\end{center}
	Siccome stiamo considerando valori reali, è possibile che $X$ non possa assumere ogni numero. In questo caso il valore della probabilità dell'esito non possibile sarà, ovviamente, zero. Agli antipodi sta la probabilità massima, data dalla somma di tutte le chances di ogni esito. Sarà uguale ad uno, quindi $\sum_{x \in R} p(x) = 1$.
	\item \textbf{Continue}; Se esiste una funzione $f(x)$ detta \textbf{densità} della variabile aleatoria tale che per ogni insieme $A \subseteqq R$ si ha:
	\begin{center}
		\[P(X \in A) = \int_A f(x) dx\]
	\end{center}
	La ragione per cui è richiesto un integrale è che questo tipo di variabile aleatoria può assumere un range di valori reali. Essendo loro pressoché infiniti, rendendo anche la probabilità di esperirne uno solo infima, è necessario considerarli come una collettività. Altri casi sono:
	\begin{center}
		\[\int^b_a f(x)dx = P(a \leqq X \leqq b), \int^{+\infty}_0 f(x)dx = P(X \geqq 0), \int^{+\infty}_{-\infty} f(x) dx = 1\]
	\end{center}
\end{itemize}
\noindent Ora possiamo introdurre un nuovo concetto importante:
\begin{definition}
	\textbf{Valore atteso}\par
	\noindent Si tratta della media pesata dei possibili valori che $X$ può assumere e si scrive:
	\begin{itemize}
		\item Per variabili discrete: \[E(X) = \sum_{x \in R} xp(x) = x_1p(x_1) + x_2p(x_2) + ... + x_np(x_n)\]
		\item Per variabili continue: \[E(X) = \int^{+\infty}_{-\infty} xf(x) dx\]
	\end{itemize}
\end{definition}
\noindent Per eventuali diverse forme di X basta sostituire la forma alle $x_i$ piccole. Per esempio, sostituiremo $X^2$ come $x^2$ dove stanno tutte le $x$ nella formula.
\begin{eg}
	\textbf{Calcolo del valore atteso della variabile aleatoria discreta $X^2$}
	\[E(X^2) = \sum_{x \in R} x^2p(x) = x^2_1p(x_1) + x^2_2p(x_2) + ... + x^2_np(x_n)\]
\end{eg}

% TODO ----- INSERISCI ESEMPIO PRATICO DI CALCOLO

\noindent Questa formula viene con delle proprietà, le quali sono differenti per i due tipi di variabili in gioco:
\begin{itemize}
	\item $E(aX + b) = aE(X) + b$, dove $a,b \in \mathbb{R}$.
	\item Con $X,Y$ variabili aleatorie dipendenti da uno stesso esperimento: $E(X + Y) = E(X) + E(Y)$.
	\item Valore atteso di una funzione di variabile aleatoria: $E(g(X)) = \sum_{x \in R} g(x)p(x)$.
\end{itemize}
\noindent Per le variabili continue:
\begin{itemize}
	\item \[E(X) = \int_{-\infty}^{+\infty} xf(x) dx\]
	\item \[E(g(X)) = \int_{-\infty}^{+\infty} g(x)f(x) dx\]
\end{itemize}
\noindent In tal merito, sia una variabile aleatoria $X$ con il rispettivo valore atteso $\mu$, quindi $E(X) = \mu$. Da qui possiamo ottenere il concetto di \textbf{varianza}, ovvero il valore atteso degli scarti.
\begin{center}
	$Var(X) = E[(X-\mu)^2] = E(X^2) - \mu^2$ (grazie all'algebra)
\end{center}
\noindent Per calcolare questo valore abbiamo bisogno di due elementi: il valore atteso $E(X)$ e al quadrato $E(X^2)$. I calcoli sono diversi in base al tipo di variabile preso in esame:
\begin{enumerate}
	\item Valore atteso grado 1:\par 
	\[\mu = E(X) = \begin{cases}
		\sum_{x \in R} xp(x)\\
		\int^{+\infty}_{-\infty} xf(x)dx
	\end{cases}\]
	\item Valore atteso al quadrato:
	\[E(X^2) = \begin{cases}
		\sum_{x^2 \in R} x^2p(x)\\
		\int^{+\infty}_{-\infty} x^2f(x)dx
	\end{cases}\]
\end{enumerate}
\noindent Notare infine come ultima cosa che la varianza è un valore compreso fra $0$ e $1$.

% TODO ----- AGGIUNGI ESEMPIO CON ESERCIZIO

%

\section{Distribuzioni congiunte}
Può capitare di lavorare con variabili dipendenti da uno stesso esperimento. In tal caso avremo una funzione di massa \textbf{congiunta} scritta come:
\begin{center}
	$p_{X,Y}(x,y) = P(X = x, Y = y)$, con $X,Y$ variabili aleatorie discrete.
\end{center}
\noindent Per esempio, lanciamo un dado e definiamo le due variabili aleatorie discrete:
\begin{itemize}
	\item X = punteggio più basso.
	\item Y = punteggio più alto.
\end{itemize}
\noindent Ne deriva necessariamente che, volendo sapere la probabilità che esca un certo numero:
\begin{itemize}
	\item $P_{X,Y}(1,1) = P(X = 1, Y = 1) = \frac{1}{36}$, perché in ambo i lanci è uscito 1.
	\item $P_{X,Y}(1,2) = P(X = 1, Y = 2) = \frac{1}{18}$, perché stai sommando le probabilità che escano i numeri nell'ordine $(1,2), (2,1)$. Ciò risulta ovviamente più probabile piuttosto che due numeri singoli.
\end{itemize}
\noindent Un'altra cosa importante è che ci è possibile ottenere la funzione di massa di una singola variabile a partire da quella congiunta\footnote{Non è possibile il contrario, tuttavia. Soffri.}, infatti:
\begin{itemize}
	\item $P(X = x) = p(x) = \sum_{y \in \mathbb{R}} p_{X,Y}(x,y)$
	\item $P(Y = y) = p(y) = \sum_{x \in \mathbb{R}} p_{X,Y}(x,y)$
\end{itemize}
\noindent In questo contesto, le funzioni di massa $p_X, p_Y$ sono dette \textbf{marginali}. Come ben penserai, è possibile calcolare il valore atteso di una funzione di due variabili aleatorie nel seguente modo:
\begin{center}
	\[E[g(X,Y)] = \sum_{x,y \in R} g(x,y)p_{X,Y}(x,y)\]
\end{center}
\begin{definition}
	\textbf{Variabili aleatorie indipendenti}\par
	\noindent Due variabili aleatorie si dicono tali se valgono le seguenti relazioni:
	\begin{enumerate}
		\item $P(X \in A, Y \in B) = P(X \in A)P(Y \in B)$.
		\item $P(X \in A | Y \in B) = P(X \in A)$, equivalente alla prima.
		\item $P(X \in B | Y \in A) = P(X \in B)$, equivalente alla prima.
		\item $P_{X,Y}(x,y) = p_X(x)p_Y(y)$.
	\end{enumerate}
\end{definition}
\noindent Siano ora due variabili aleatorie $X,Y$, definiamo il valore di \textbf{covarianza} come:
\begin{center}
	$Cov(X,Y) = E[(X-E(X))(Y-E(Y))] = E(XY) - E(X)E(Y)$
\end{center}
\noindent Abbiamo inoltre le seguenti relazioni:
\begin{itemize}
	\item Uguaglianza vera: $Cov(X,X) = Var(X)$.
	\item Se le due variabili aleatorie sono indipendenti e quindi \textbf{scorrelate}\footnote{Notare che se due variabili sono scorrelate, non necessariamente ne implica anche l'indipendenza.}: $Cov(X,Y) = 0$
\end{itemize}
\noindent Inoltre, se il valore di covarianza è positivo, allora le variabili saranno positivamente correlate, mentre se è negativo varrà il contrario.
\begin{eg}
	\textbf{Quando una variabile è scorrelata o indipendente?}\par 
	\noindent Prendiamo un'urna con due palline numerate con $1,2$ e definiamo due variabili aleatorie $X_1, X_2$ che avranno il loro valore pescato.
	\begin{itemize}
		\item Se le estrazioni avvengono con reimmissione allora $X_1, X_2$ saranno indipendenti e quindi scorrelate.
		\item Se le estrazioni avvengono senza reimmissione, le variabili saranno scorrelate, ma non indipendenti, perché la prima estrazione ha influenzato la seconda. Infatti per calcolare la covarianza useremo la funzione di massa congiunta.
	\end{itemize}
\end{eg}
\noindent Questo concetto introduce quello di \textbf{coefficiente di correlazione}, un valore $Corr(X,Y) \in [-1, 1]$ che si calcola nel seguente modo:
\begin{center}
	\[Corr(X,Y) = \dfrac{Cov(X,Y)}{\sqrt{Var(X)}\times\sqrt{Var(Y)}}\]
\end{center}
\noindent Inoltre, avremo i due seguenti casi specifici:
\begin{itemize}
	\item $Corr(X,Y) = 1 \iff \exists(a>0 \land b) \in \mathbb{R}|Y = aX+b$ 
	\item $Corr(X,Y) = -1 \iff \exists(a<0 \land b) \in \mathbb{R}|Y = aX+b$  
\end{itemize}
\noindent Esiste anche la \textbf{funzione di ripartizione}, utile per calcolare la probabilità che una variabile aleatoria sia minore di un dato evento e per capire le modalità di crescita delle altre variabili aleatorie. Questo valore non è mai decrescente ed è definito (per una variabile aleatoria $X$) con:
\begin{center}
	$F_X(x) = P(X \leq x)$
\end{center}
\noindent Generalmente abbiamo le due scritture per i due tipi di variabile aleatoria:
\begin{itemize}
	\item Per le discrete:
	\begin{center}
		$F_X(x) = \sum_{t \leq x} p_X(t) \implies p_X(x) = \Delta F_X(x) = F_X(x) - lim_{t \to x^+} F_X(t)$.
	\end{center}
	\item Per le continue: 
	\begin{center}
		$F_X(x) = \int^x_{-\infty} f_X(t) dt \implies F'_X(x) = f_X(x)$.
	\end{center}
\end{itemize}
\begin{eg}
	Abbiamo due variabili aleatorie $X,Y$ definite come funzione lineare di $X$, quindi $aX+b$. Si calcoli la densità di $Y$ avendo a disposizione quella di $X$. Eseguiamo i seguenti passaggi:
	\begin{enumerate}
		\item $F_Y(y) = P(Y \leq y) = P(aX + b \leq y) = P(X \leq \frac{y-b}{a}) = F_X(\frac{y-b}{a})$.
		\item Deriviamo, ottenendo: $F'_Y(y) = f_Y(y) = (\frac{y-b}{a})\times \frac{1}{a} = \frac{1}{a}f \times (\frac{y-b}{a})$
	\end{enumerate}
\end{eg}
\noindent In tal merito, abbiamo altre due proprietà della varianza:
\begin{itemize}
	\item $Var(aX+b) = a^2Var(X)$.
	\item $Var(X+Y) = Var(X) + Var(Y) + 2Cov(X,Y)$.
	\item Se le variabili aleatorie sono scorrelate: $Var(X+Y) = Var(X) + Var(Y)$.
\end{itemize}

%

\section{Classi notevoli di variabili aleatorie}
Dentro ai due tipi di variabili aleatorie ne possiamo riconoscere alcune con delle dinamiche notevoli e tocca impararle perché le chiede all'esame. Iniziamo:
\begin{itemize}
	\item \textbf{Variabili aleatorie di Bernoulli}\par 
	\noindent Una variabile aleatoria di questo tipo di parametro $p \in [0,1]$ se assume solamente i valori $0,1$. Inoltre:
	\begin{itemize}
		\item Probabilità: $P(X = 1) = p = 1-P(X = 0) \implies p_X(1) = p \land p_X(0) = 1-p$.
		\item Valore atteso: $E(X) = 0*(1-p) + 1p = p \implies E(X^2) = E(X) = p$.
		\item Varianza: $Var(X) = E(X^2) - E^2(X) = p-p^2 = p(1-p)$.
	\end{itemize}
	\item \textbf{Variabili aleatorie binomiali}\par
	\noindent Consideriamo un numero di prove ripetute indipendenti con probabilità di successo $p$ e sia $X$ la variabile aleatoria del numero di successi.\par 
	Allora questa potrà assumere i valori da $0$ a $n$. Definiamo la variabile $X_i$ come:
	\begin{center}
		$X_i = \begin{cases}
			1\\
			0
		\end{cases}$
	\end{center}
	\noindent Dove risulta $1$ se la prova ha esito positivo, altrimenti è uguale a $0$. È fondamentalmente una sommatoria di variabili di Bernoulli. Inoltre:
	\begin{itemize}
		\item Funzione di massa: $p_X(k) = P(X = k) = \begin{pmatrix}
			n\\
			k
		\end{pmatrix}p^k(1-p)^{n-k}$
		\item Valore atteso: $E(X) = \sum^n_{i = 1}E(X_i) = np$
		\item Varianza: $Var(X) = \sum^n_{i = 1}Var(X_i) = np(1-p)$
	\end{itemize}
	\item \textbf{Variabili aleatorie di Poisson}\par 
	\noindent Una variabile aleatoria $X$ di parametro $\lambda > 0$ è di questo tipo se può assumere i valori $(0,1,...,n,...)$. Si utilizza per calcolare la probabilità di eventi rari in un totale di esperimenti ed è illimitata.\par 
	Facciamo esempio che si effettui un numero spropositato di esperimenti aleatori, sui quali la probabilità che avvenga un evento $X$ è infima, quasi impossibile. In tal caso, $X$ sarà considerabile come una \textbf{variabile aleatoria di Poisson} di parametro $np$.\newline
	
	\noindent Queste variabili aleatorie hanno anche la caratteristica di poter essere approssimate nel tipo binomiale $Y$ visto prima di parametri $n$ e $\frac{\lambda}{n}$, con $n >> 1$. Si può vedere con un valore approssimato del valore atteso e della varianza, come segue.
	\begin{itemize}
		\item Funzione di massa: $p_X(n) = P(X = n) = e^{-\lambda} \frac{\lambda^n}{n!}$
		\item Valore atteso: $E(X) \approx E(Y) = n\times\frac{\lambda}{n} = \lambda$
		\item Varianza: $Var(X) \approx Var(Y) = n\times\frac{\lambda}{n}(1-\frac{\lambda}{n}) \approx \lambda$
	\end{itemize}
	\noindent È inoltre dimostrabile che $E(X) = \lambda = Var(X)$, la quale può dimostrarsi una relazione utile da ricordare.
	\item \textbf{Variabili aleatorie uniformi}\par 
	\noindent Siano $\alpha < \beta$ due numeri reali. $X$ è una variabile aleatoria uniforme in $(\alpha,\beta)$ se vale:
	\[f_X(x) = \begin{cases}
		\alpha \leq x \leq \beta \implies \frac{1}{\beta - \alpha}\\
		0
	\end{cases}\]
	\begin{itemize}
		\item Valore atteso: $\int^{+\infty}_{-\infty} xf(x)dx = \int^\beta_\alpha \frac{x}{\beta-\alpha}dx = \frac{\alpha + \beta}{2}$
		\item Valore atteso$^2$: $E(X^2) = \int^{+\infty}_{-\infty} x^2f_X(x)dx = \int^\beta_\alpha \frac{x^2}{\beta-\alpha}dx = \frac{\alpha^2 + \alpha\beta + \beta^2}{3}$
		\item Varianza: $Var(X) = E(X^2) - E^2(X) = \frac{(\beta - \alpha)^2}{12}$
	\end{itemize}
	\noindent Una particolarità è come il valore atteso risulta essere il punto medio dell'intervallo considerato.
	\item  \textbf{Variabili aleatorie normali}\par 
	\noindent Fissiamo $\mu \in \mathbb{R}$ e $\sigma^2 > 0$. Diciamo che $X$ è una tale variabile aleatoria di parametri $\mu e \sigma^2$ e scriviamo:
	\begin{center}
		$X \sim N(\mu, \sigma^2)$ se $f_X(x) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$
	\end{center}
	\noindent Questa è la densità che caratterizza la curva gaussiana, di media $\mu$ e varianza $\sigma^2$; infatti, la funzione $f(x) = e^{-x^2}$ forma una curva gaussiana con un dato centro, spostabile modificando il valore $\mu$. 
	
	% TODO ----- Inserisci immagine della gaussiana
	\begin{center}
		INSERIRE IMMAGINE
	\end{center}
	\noindent Notare inoltre che se una variabile aleatoria $Z \sim N(0,1)$, diremo che la prima è una \textbf{normale standard} e la sua funzione risulta essere: $f_Z(x) = \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}$; inoltre:
	\begin{itemize}
		\item Valore atteso: $E(Z) = 0$
		\item Varianza: $Var(Z) = 1$
	\end{itemize}
	\noindent Ritornando invece al caso generale, ovvero $\mu \in \mathbb{R} e \sigma^2 > 0$ abbiamo che:
	\begin{itemize}
		\item Variabile aleatoria: $X = \sigma Z + \mu \sim N(\mu,\sigma^2)$
		\item Valore atteso: $E(X) = \sigma E(Z) + \mu = \mu$
		\item Varianza: $Var(X) = \sigma^2Var(Z) = \sigma^2$
	\end{itemize}
	\noindent Ogni variabile aleatoria di questo tipo è facilmente trasformabile in una normale standard mediante la \textbf{standardizzazione}: 
	\[\frac{X-\mu}{\sigma} \sim N(0,1)\]
	\noindent Sapendo questo, dichiariamo le seguenti due variabili aleatorie normali indipendenti:
	\[X \sim N(\mu_1, \sigma_1^2), Y \sim N(\mu_2, \sigma_2^2) \implies X+Y \sim N(\mu_1+\mu_2, \sigma_1^2+\sigma_2^2)\]
	\noindent Il calcolo della probabilità di queste variabili si effettua con la standardizzazione. Calcoliamo $P(a \leq X \leq b)$:
	\begin{equation}
		\begin{split}
			P(a \leq X \leq b) &= P(\frac{a - \mu}{\sigma} \leq \frac{X-\mu}{\sigma} \leq \frac{b-\mu}{\sigma})\\
				&= P(\frac{a-\mu}{\sigma} \leq Z \leq \frac{b-\mu}{\sigma})\\
				&= \phi(\frac{b-\mu}{\sigma}) - \phi(\frac{a-\mu}{\sigma})
		\end{split}
	\end{equation}
	\noindent Dove $\phi(x) = F_Z(x)$ è la funzione di ripartizione $Z \sim N(0,1)$. Un'ultima cosa utile da ricordare è che, siccome stiamo esaminando una curva gaussiana, per la valutazione dell'integrale è possibile fare $1-\int$ oppure, siccome è sicuramente una funzione pari, si può specchiare il punto richiesto e calcolare in sua funzione... oppure puoi scegliere il metodo sano ed utilizzare la \textbf{tabella di probabilità}.
	
	% TODO ----- INSERISCI IMMAGINE TABELLA DI PROB.
	
	% TODO ----- INSERISCI ESEMPIO DI USO STANDARDIZZAZIONE CON TABELLA (6,7)
	
	\begin{note}
		\textbf{Istruzioni per l'utilizzo della tabella}\par 
		\noindent Vogliamo calcolare la probabilità di $Z = 1.52$. Nota che sulla colonna all'estrema sinistra è indicato un intervallo di valori; prendi quello che più si avvicina a quello richiesto. Bisogna poi trovare un numero nella riga in cima alla tabella sicché la loro somma dia il valore di $Z$. La casella che interseca questa riga e colonna sarà il numero che ci serve.
	\end{note}
\end{itemize}

%

\section{Statistiche campionarie}
Siano $X_1, ..., X_n$ variabili aleatorie indipendenti ed identicamente distribuite. Queste sono un modello per un dataset o un campione $n$ di dati. Ogni combinazione di tali variabili si dice \textbf{statistica campionaria}. In particolare, abbiamo le due seguenti:
\begin{itemize}
	\item \textbf{Media campionaria}: \[\overline{X_n} = \frac{1}{m}(X_1 + ... + X_m)\]
	\item \textbf{Varianza campionaria}: \[S^2_m = \frac{1}{m-1}\sum^m_{i=1}(X_i - \overline{X_n})^2\]
\end{itemize}
\noindent Tenderemo a considerare solo la media campionaria, in quanto la statistica più semplice con la quale lavorare. Più nello specifico, ricaviamo i suoi valori di:
\begin{itemize}
	\item \textbf{Valore atteso}: \[E(\overline{X}_n = \frac{1}{m}(E(X_1, ..., E(X_m))) = \mu\]
	\item \textbf{Varianza}: \[Var(\overline{X}_n) = \frac{1}{n^2}\sum_{i=1}^{n}Var(X_i) = \frac{n\sigma^2}{n^2} = \frac{\sigma^2}{n}\]
\end{itemize}
\noindent Inoltre, per avere ulteriori informazioni sulla distribuzione di $\overline{X_n}$, consideriamo il \textbf{teorema del limite centrale}:
\begin{theorem}
	\textbf{Teorema del limite centrale}\par 
	\noindent Qualunque sia la distribuzione delle $X_i$, la distribuxione di $Z_n$ per $n$ grande è ben approssimata da una $N(0,1)$.
	\[Z_n = (\overline{X_n}-\mu) \frac{\sqrt{m}}{\sigma}\sim N(0,1) \equiv \overline{X_n}\sim N\left(\mu, \frac{\sigma^2}{n}\right)\]
	\noindent Più precisamente, $\forall z \in \mathbb{R} \land n \to +\infty$:
	\[F_{Z_n}(z) = P(Z_n \leq z) \to \phi(z)\]
\end{theorem}
\noindent Prestiamo particolare attenzione all'applicazione del teorema al caso $X_i$ di Bernoulli di parametro $p$, ovvero \[X_1+ ...+ X_n = X\] con $X$ binomiale di parametri $n,p$. Il teorema del limite centrale afferma che:
\[X \sim N(np, np(1-p))\]
\noindent Questa approssimazione si dice \textbf{normale per una binomiale} ed è considerata buona se rispetta la seguente restrizione:
\[np \geq 5, n(1-p) \geq 5\]
\noindent Usando questa stima, essendo che si sta calcolando un valore di probabilità approssimando una distribuzione discreta con una continua, si deve applicare la \textbf{correzione di continuità}; una modifica dell'intervallo di integrazione ai suoi estremi di $\frac{1}{2}$.\par 
Per esempio, data una variabile aleatoria $X$ con distribuzione binomiale di parametri $n,p$, per $n$ sufficientemente grande, si può assumere che:
\[P(X \leq x) \approx P\left(Y \leq x+\frac{1}{2}\right)\]
\noindent Dove $Y$ è una variabile aleatoria che segue una distribuzione normale con parametri $\mu = np$ e $\sigma^2 = np(1 - p)$. Ne risulta che la correzione dell'approssimazione è molto maggiore, rispetto a non utilizzarla.

% TODO Inserisci esercizio di esempio



%--- Ex.8 - Rivedi a casa
%Lanciamo 100volte una moneta equilibrata. Qual è prob che il numero d teste sia >= 40 e <= 70?
%Usiamo il teorema del limite centrale insieme alla correzinoe di continuità:

%Abbiamo la VA e dico che si distribuisce con una N(50,25).
%P(40 \leq X \leq 70) \approx P(39.5 \leq X \leq 70.5)

%--- Esercizio 9
%Quante volte almeno devo lanciare una moneta normale in modo da otwenere più di 50 con probabilità maggiore al 95%?
%Stabiliamo che X = X_1 + X_2 + ... + X_n è una VA binomiale di parametri n e 1/2. Sere trovare un n tale per cui: P(X > 50) \geq 0,95

%Con corr. di continuità diventa P(X > 50,5)
%Con X VA di media n/2 = \mu e varianza n/4 = \sigma^2.

%Il valore cercato standardizzato è:
%P(\frac{X-\mu}{\sqrt{\sigma^2}} > \frac{50,5 - \mu}{\sqrt{\sigma^2}}) controlla poi la fine, è un casino assurdo.

%--- Esercizio 1 scritto in comic sans (porcodio)
%P(X \leq 4,9)				Qua normalizzo: P(\frac{X-5,7}{1} = \frac{4,9-5,7}{1}) = Z
%P(4,9 \leq X \leq 6,2)
%P(X \geq x) = 0,40

%P(Z \leq -0,8)
%Qui vediamo la tabellina e cerchiamo 0,8, che è uguale a 0,7881, perché non abbiamo quello di -0,8.
%Possiamo calcolare 1-P(Z \leq 0,8) = 0,2119

%Quindi avremo P(-0,8 \leq Z \leq 0,5) \implies P(Z \leq 0,5) - P(Z \leq -0,8) = 0,48

%Invece per P(X \geq x) = 0,4 = \phi(Y \leq L)
%Bisogna trovare la L per cui vale la relazione.