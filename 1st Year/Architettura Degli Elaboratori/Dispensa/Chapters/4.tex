\section{Modello di Von Neumann e Unità funzionali del calcolatore}
Il \textbf{Modello di Von Neumann} è un'architettura composta da tre componenti principali interconnesse mediante un dispositivo detto \textbf{BUS}\footnote{Flusso di bits che rende possibile la comunicazione fra le varie componenti}. Si tratta del modello sul quale si basano tutte le architetture dei calcolatori. Le sue parti sono:\newline

\begin{minipage}{0.475\textwidth}
	\includegraphics[width=1\linewidth]{Images/VonNeumannArch.png}
	\label{fig:VonNeumannArch}
\end{minipage}
\vspace{0.02\textwidth}
\begin{minipage}{0.475\textwidth}
	\begin{itemize}
		\item \textbf{Processore}: Atto all'elaborazione dei dati.
		\item \textbf{Memoria}: Atta al salvataggio dei dati.
		\item \textbf{Dispositivi I/O}: Periferiche varie come microfoni o tastiere.
	\end{itemize}
\end{minipage}
\noindent Più nello specifico, un calcolatore elabora i dati grazie all'ausilio delle seguenti tre componenti, ognuna indipendente dall'altra:
\begin{itemize}
	\item \textbf{I/O}: Unità di ingresso ed uscita.
	\item \textbf{ALU}: Unità aritmetico-logica.
	\item \textbf{CU}: Control Unit, che compone processore e memoria.
\end{itemize}
\noindent Le prime due lavorano sotto supervisione e controllo della \textbf{CU}. L'unità di input riceve informazioni in forma codificata da operatori o periferiche, le quali saranno elaborate dalla \textbf{ALU} per effettuare calcoli, eventualmente salvando in memoria i risultati. Quanto eseguito verrà inviato all'unità di output, la quale ritornerà il tutto.\par
Le informazioni manipolate sono categorizzate in \textbf{istruzioni} e \textbf{dati}; le prime sono comandi dati alla macchina, direttamente interpretabili da essa, mentre i secondi sono le informazioni che vengono manipolate. Una lista di istruzioni compone il \textbf{programma}, il quale potrà svolgere algoritmi elaborando i dati. Se in esecuzione, si troverà sempre in memoria, a meno che non venga dato un comando di interruzione.\newline

\noindent L'ambiente di lavoro del corso sarà la CPU Intel 80x86, con la particolarità di utilizzare lo stesso linguaggio del microprocessore: \textbf{Assembly}. L'insieme che compone le istruzioni scritte in tale lingua leggibili dal microprocessore si dice \textbf{ISA}, Instruction Set Architecture, ed è letto da un \textbf{assemblatore}, il quale provvederà a tradurlo in codice oggetto.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{Images/CPUArch.png}
	\caption{Architettura di una CPU}
	\label{fig:CPUArch}
\end{figure}
\noindent Analizziamo ora le microcomponenti della CPU non ancora viste:
\begin{itemize}
	\item \textbf{Componenti generali}:
	\begin{itemize}
		\item \textbf{Bus Dati}: Consente di trasportare i dati fra le varie componenti.
		\item \textbf{Bus Indirizzi}: Comunica gli indirizzi di memoria delle informazioni.
		\item \textbf{Bus di Controllo}: Invia i segnali di controllo fra le varie componenti.
		\item \textbf{Memory Address Register}: Tiene in memoria e fornisce gli indirizzi dei dati da manipolare.
		\item \textbf{Memory Data Register}: Salva temporaneamente i dati da o per la CPU.
	\end{itemize}
	\item \textbf{Componenti della control unit}:
	\begin{itemize}
		\item \textbf{Instruction Register}: Contiene gli identificativi delle istruzioni.
		\item \textbf{Program Counter}: Contiene gli indirizzi delle stesse.
		\item \textbf{Program Status Word}; Insieme di flags che, in stretta collaborazione con la ALU, indicano lo stato dei diversi risultati di operazioni matematiche. Si modifica ad ogni singola operazione.
	\end{itemize}
\end{itemize}
\noindent Ora che sappiamo da cosa è composta, è il momento di chiederci come funziona questa CPU. Ci è possibile descrivere tale processo mediante una FSM a tre stati, chiamati \textbf{Fetch}, \textbf{Decode} ed \textbf{Execution}.\par
Il primo rappresenta la ricezione delle informazioni. Ottiene i dati dall'\textbf{MDR}, memory data register, per poi passare tutto all'\textbf{IR}, instruction register con il bus dati. Nel frattempo, il program counter aumenterà di 1 ad ogni istruzione ricevuta.\par
Il secondo stato è la fase di decodifica delle istruzioni; tramite l'ISA della macchina, si indirizzano opportunamente i dati che verranno poi elaborati.\par
Il terzo ed ultimo stato è infatti quello dell'esecuzione delle istruzioni decodificate.\newline
\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\linewidth]{Images/FetDecExe.png}
	\caption{Macchina a stati della CPU}
	\label{fig:FDE}
\end{figure}

\noindent L'ISA, come si può facilmente dedurre, non è uguale per ogni singolo calcolatore, tuttavia presenta sempre lo stesso scheletro, composto da:
\begin{itemize}
	\item \textbf{OPcode}: Il codice operazione. Comunica quante istruzioni possono essere registrate, ma soprattutto quale sta venendo effettuata.
	\item \textbf{Source Address}: L'indirizzo dal quale ottenere le informazioni.
	\item \textbf{Destination Address}: L'indirizzo nel quale verranno salvate le informazioni.
\end{itemize}
\noindent La modalità di scambio e trasmissione dati si chiama \textbf{indirizzamento}, i cui tipi sono discussi più nello specifico nella sezione relativa ad Assembly.

%

\section{Architettura CPU RISC-V}
Come precedentemente menzionato, le CPU organizzano il lavoro del calcolatore, e più nello specifico ciò avviene grazie ad un ciclo di \textbf{fetch}, \textbf{decode} ed \textbf{execution}. Segue i passi:
\begin{enumerate}
	\item Il program counter si modifica per puntare all'istruzione successiva.
	\item Viene determinato il tipo dell'istruzione letta.
	\item Se l'istruzione usa una word in memoria, si determina dove essa si trovi.
	\item Se necessario, si carica la word in un registro della CPU.
	\item Esecuzione dell'istruzione.
	\item Ripeti il passo 1.
\end{enumerate}
\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\linewidth]{Images/ControlUnit_simple.png}
	\caption{Architettura semplificata della control unit}
	\label{fig:CU_simple}
\end{figure}
\noindent A partire da questo disegno, è necessario andare più nel dettaglio con alcune componenti. Per esempio, è importante notare i due adder in alto. Questi consentono di spostarsi fra le varie istruzioni; infatti il primo all'estrema sinistra è quello che aggiorna il program counter sommandogli $4B$, spostandolo all'istruzione successiva, mentre il secondo adder è utile per le istruzioni di salto. Ottenuto il nuovo indirizzo, il program counter verrà aggiornato.\par
Bisogna inoltre tenere a mente che i dati da passare alla ALU sono contenuti nei \textbf{registri}; in base al segnale dato dalla control unit, si effettuerà un'operazione specifica. Detto ciò, possiamo elaborare sul ciclo di elaborazione informazioni:
\begin{itemize}
	\item \textbf{Fetch}, la selezione della parola corrispondente all'istruzione da eseguire. Qui il program counter fornisce l'indirizzo di memoria in cui si trova la prossima istruzione. Questo valore è ottenuto da una memoria read only chiamata \textbf{memoria delle istruzioni}.
	\item \textbf{Decode}, la decodifica dell'istruzione in codice oggetto. In primo luogo la ALU decodifica le istruzioni ricevute in base al valore del program counter e, se necessario, verranno caricati gli operandi dalla \textbf{memoria dei dati}.
	\item \textbf{Execution}, l'esecuzione dell'istruzione. Qui si possono effettuare processi diversi, come:
	\begin{itemize}
		\item Eseguire un calcolo con la ALU.
		\item Elaborare il contenuto degli operandi e determinare un indirizzo di memoria.
		\item Eseguire un confronto per effettuare dei salti.
	\end{itemize}
\end{itemize}
\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\linewidth]{Images/ControlUnit1.png}
	\caption{Architettura della control unit}
	\label{fig:CU1}
\end{figure}
\noindent Notare adesso nella nuova figura con aggiunta la \textbf{control unit} che i segnali dei due adder vanno in un mux, per capire quale utilizzare in base al segnale branch, ovvero di salto, usato come controllo. Quest'ultimo è dato da una porta AND che prende:
\begin{itemize}
	\item Il risultato della condizione per il jump (zero).
	\item Il segnale di richiesta di jump (uscito dalla CU).
\end{itemize}
\noindent Se e solo se entrambi i bits sono veri, si va a saltare all'indirizzo richiesto, ed in tal caso il primo parametro è dato dal PC, mentre il secondo è un pezzo corrispondente al path saltato.\par
Un altro compito della control unit è l'invio di segnali \textbf{memoryRead},  \textbf{memoryWrite} e \textbf{registerWrite}, i quali consentono di leggere e scrivere valori in memoria.\par
Notare inoltre il multiplexer posto fra la ALU ed il blocco centrale; è necessario per scegliere il luogo da dove prendere l'operando. È possibile ottenerlo dalla memoria oppure direttamente dall'input.\newline

\noindent Adesso invece andiamo a vedere nello specifico il comportamento del datapath in base all'istruzione data. Ne abbiamo di quattro tipi:
\begin{itemize}
	\item \textbf{Type-R}: Istruzioni aritmetico-logiche.\par
	I registri \textbf{rs1} e \textbf{rs2} presentano il numero dei registri sorgenti ed \textbf{rd} contiene il numero del registro di destinazione. L'operazione da eseguire sta nei campi \textbf{func3} e \textbf{func7} ed è letto dalla control unit per comunicare i segnali adatti alla ALU.
	\item \textbf{Type-L}: Istruzioni di caricamento. Necessita di memoria dati e componente per estensione del segno.\par 
	Si attiva con il segnale \textbf{memRead} a valore vero. Qui \textbf{rs1} è il registro base il cui contenuto è sommato al campo immediato di 12b per ottenere l'indirizzo del dato in memoria. Il campo \textbf{rd} è il registro destinazione per il valore letto.
	\item \textbf{Type-S}: Istruzioni di salvataggio. Necessita di memoria dati e componente per estensione del segno.\par 
	Si attiva con il segnale \textbf{memWrite} a valore vero. Qui \textbf{rs1} è il registro base il cui contenuto è sommato al campo immediato di 12b per ottenere l'indirizzo del dato in memoria. Il campo \textbf{rs2} è il registro sorgente il cui valore è poi copiato in memoria.
	\item \textbf{Type-SB}: Istruzioni di salto.\par
	\textbf{rs1} e \textbf{rs2} sono confrontati. Il campo immediato di 12b è preso, il suo bit di segno viene esteso, shiftato a sinistra di una posizione e sommato al program counter per calcolare l'indirizzo di destinazione del salto.
\end{itemize}
\begin{figure}[h]
	\centering
	\includegraphics[width=1\linewidth]{Images/RISC-V_CU.png}
	\caption{Architettura della control unit RISC-V}
	\label{fig:R5-CU}
\end{figure}
\begin{eg}
	\textbf{Istruzione [sub x4, x5, x6]}
	\begin{enumerate}
		\item Il PC dà l'indirizzo dell'istruzione e viene incrementato di $4B$.
		\item L'istruzione è decodificata, viene riconosciuto il tipo-R. Ciò è indicato dal Codop, che è inviato alla CU.
		\item In base al Codop, la CU imposta i segnali: \textbf{regWrite = 1}, perché scriverà in x4, \textbf{ALUSrc = 0}, perché gli operandi vengono dai registri, \textbf{memWrite = 0}, \textbf{memRead = 0}, \textbf{memToReg = 0}, perché non c'è accesso alla memoria.
		\item La CU legge \textbf{func3=0b000} e \textbf{func7=0b0100000}, determinando che l'operazione è una sottrazione.
		\item La ALU calcola $x4 = x5-x6$.
		\item Se il risultato è $0$, il segnale \textbf{Zero} sarà vero, ed il primo è scritto nel register file.
		\item Adesso $x4$ contiene il valore $x5-x6$.
	\end{enumerate}
\end{eg}
\begin{eg}
	\textbf{Istruzione [and x7, x8, x9]}
	\begin{enumerate}
		\item PC dà l'indirizzo dell'istruzione e si incrementa di $4B$.
		\item Il decoder la riconosce come Type-R.
		\item La CU imposta: regWrite = 1, ALUSrc = 0, memWrite = 0, memRead = 0, memToReg = 0.
		\item La ALU ottiene \textbf{func3=0b111} e \textbf{func7=0b0000000}, selezionando l'operazione logicalAND. Esegue quindi $x7 = x8 \land x9$.
		\item Scrive il risultato nel register file.
		\item $x7$ contiene il risultato di $x8 \land x9$.
	\end{enumerate}
\end{eg}
\begin{eg}
	\textbf{Istruzione [lw x4, 16(x5)]}
	\begin{enumerate}
		\item PC dà l'indirizzo dell'istruzione e avanza di $4B$.
		\item L'istruzione decodificata è type-L, LOAD.
		\item $x5$ viene letto dal register file per ottenere l'indirizzo base.
		\item L'offset $16$ viene estratto e inviato alla ALU, che calcola l'indirizzo effettivo eseguendo $x5+16$.
		\item L'indirizzo risultante è inviato alla memoria dati, che restituisce il valore ivi memorizzato.
		\item Il valore letto va nel mux che riceve anche memToReg=1 e regWrite=1.
		\item Il valore è ora scritto in $x4$, che conterrà infatti il valore memorizzato all'indirizzo $x5+16$.
	\end{enumerate}
\end{eg}
\begin{eg}
	Inserisci esempio istruzione di salto.
\end{eg}
\noindent Un'ultima particolarità di cui tener conto è il quantitativo di bits usati per un'operazione. Usiamo load come esempio; avremo:
\begin{itemize}
	\item \textbf{lw}: Load word.
	\item \textbf{lh}: Load half-word.
	\item \textbf{lb}: Load byte.
	\item \textbf{l*u}: Load unsigned, con $* \in \{w, h, b\}$
\end{itemize}

%

\section{Metodi di I/O, Segnale Interrupt}
Lo scopo dei dispositivi Input/Output, detti anche \textbf{periferiche}, è quello di effettuare uno scambio di dati più naturale fra persona e macchina. Alcuni esempi di queste architetture sono tastiera, mouse o altoparlanti. Sono capaci di codificare l'informazione e mandarla al sistema mediante l'utilizzo di due registri da $1B$ l'uno.
\begin{minipage}{0.475\linewidth}
	\includegraphics[width=0.4\linewidth]{Images/Device_IO.png}
	\label{fig:IODEV}
\end{minipage}
\vspace{0.02\linewidth}
\begin{minipage}{0.475\linewidth}
	Osserviamo le loro componenti:
	\begin{itemize}
		\item \textbf{Micro-Controllore}: Piccola CPU dedicata al dispositivo. Supervisiona e controlla qualunque cosa si faccia.
		\item \textbf{Registro Dati}: Dove sono salvate tutte le codifiche della periferica. L'input è ricevuto attivamente e viene tradotto dall'interfaccia.
		\item \textbf{Registro Stato}: Effettua una funzione analoga al PSW ed esattamente come lui, ogni bit ha significato.
		\item \textbf{Interfaccia Analogico-Digitale}: Componente che traduce da segnale analogico a segnale digitale.
	\end{itemize}
\end{minipage}
I dispositivi I/O sono \textbf{Memory Mapped}; ciò significa che nella macchina intera esiste un intervallo di indirizzi riservato a loro a cui rispondono i registri Dati e Stato. Nel caso in cui si provasse a far accedere la memoria in quei registri, la CPU si rifiuterebbe. L'unico modo per entrarvi è utilizzare gli accessi da SuperUser o Admin, dipendentemente dal sistema operativo che si usa.\par
Ottenendo questi accessi, è possibile effettuare una \textbf{Supervisor Call}\footnote{SVC, Chiamata per passare il controllo delle operazioni al sistema operativo}. Ciò apre varie possibilità di personalizzazione, come la modifica degli output dei tasti in una tastiera. Se si vuole invece solo gestire le periferiche, si utilizza la tecnica del \textbf{Polling}\footnote{Verifica ciclica dei dispositivi I/O mediante testing dei bits di bus di ogni periferica, seguita da un'interazione Read/Write.}, implicando che la periferica abbia una potenza simile se non uguale a quella della CPU. In caso contrario, verrebbero persi dati in corso d'opera.\newline

\noindent Parliamo ora invece di \textbf{Interrupt}; un segnale asincrono che interrompe il lavoro della CPU. Quando il suo flag è disattivato, è richiesto l'intervento dell'unità di controllo per riattivarlo.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\linewidth]{Images/interrupt.png}
	\caption{Funzionamento dell'Interrupt}
	\label{fig:Intpt}
\end{figure}
Nello specifico, l'algoritmo si realizza tramite due elementi; \textbf{interrupt request}, dato dal dispositivo, aggiorna il valore di interrupt a $0$, ed \textbf{interrupt acknowledgement}, un segnale proveniente dalla CPU per confermare quale dispositivo ha richiesto l'interruzione. È grazie a quest'ultimo segnale che l'unità di controllo è capace di controllare sequenzialmente le richieste di ogni dispositivo. Una volta trovato il chiamante, chiamerà la \textbf{interrupt service routine} ad esso associato.\par
Il compito della ISV è salvare le modifiche fatte a PC e PSW per poi interrompere il programma; infine interverrà il microprocessore per scaricare quanto appena salvato e tornare allo stato precedente. Notare inoltre che le richieste di interrupt non si sovrappongono e sono state create per gestire tempi umani, quindi non si otterranno ulteriori richieste finché la prima non sarà risolta.\par
La ISV è poi parte integrante del \textbf{Device driver}, un programma con lo scopo di ottimizzare e ridurre gli sforzi della CPU legati al funzionamento di una periferica. Sarà discusso più nel dettaglio nella sezione apposita.

%

\section{Direct Memory Access, BUS e Arbitraggio}
Iniziamo dando una visione più vasta del problema; è nostro volere trasportare una grande quantità di dati utilizzando il sistema appena visto con l'interrupt signal. Nel modello di Von Neumann avremo di conseguenza il seguente processo:
\begin{enumerate}
	\item I dispositivi I/O ricevono l'input e lo inviano alla CPU.
	\item Ricevuti i dati, l'interrupt signal sarà posto a $0$ e si inizierà a lavorare con quanto ottenuto.
	\item I dati elaborati sono salvati in memoria.
\end{enumerate}
\noindent Capiamo subito il fatto che se l'interrupt signal è gestito in questo modo avremo uno spreco di risorse non indifferente; una soluzione al problema è ottenuta tramite il \textbf{Direct Memory Access}.\par
La CPU programmerà un dispositivo I/O per far sì che questo possa accedere direttamente alla memoria senza passare da essa. Sarà inoltre in grado di eseguire operazioni di lettura e scrittura, grazie al suo micro-controllore e relativi registri.\par
Il vantaggio sta nella divisione del lavoro fra CPU e periferiche, risparmiando energia della CPU; infatti, una volta finito il lavoro della periferica con DMA, il microprocessore della macchina dovrà ricevere un singolo interrupt. Tuttavia, ciò fa sorgere un ulteriore problema: più CPU potrebbero voler accedere allo stesso BUS e se questo avesse luogo, si potrebbero perdere dati. La soluzione sta in una gestione del BUS tramite un \textbf{arbitro}, il cui ruolo è tipicamente vestito dalla CPU principale. Sarà lei a scegliere chi e quando potrà accedere ed usare il BUS. Lo schema non è dissimile dal lavoro che esegue il singolo interrupt signal.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\linewidth]{Images/ArbitraggioBUS.png}
	\caption{Arbitraggio dei BUS}
	\label{fig:arbBUS}
\end{figure}
Nell'immagine è possibile notare la presenza di segnali nuovi:
\begin{itemize}
	\item \textbf{BUS Busy}: Segnale che indica se il BUS è attualmente utilizzato da una componente.
	\item \textbf{BUS Request}: Segnale che indica una richiesta fatta alla CPU per poter utilizzare il BUS.
	\item \textbf{BUS Grant}: Segnale di concessione di utilizzo BUS una volta terminato il precedente lavoro\footnote{La CPU trasmette questo segnale in ordine fissato, da sinistra a destra. Per ottimizzare i tempi, è intelligente porre per prime le componenti più utilizzate dal sistema}.
\end{itemize}
\noindent Prima di andare nel dettaglio è necessario familiarizzare con due termini: \textbf{Master} e \textbf{Slave}, le entità sulle quali si basa il funzionamento del BUS. Il primo è colui che inizia l'operazione, mentre il secondo ne risponde. Esistono due protocolli di operazione che renderanno rispettivamente un BUS sincrono o asincrono.\par
Negli schemi seguenti verranno usati esagoni per rendere la scrittura più compatta. Se le linee si incrociano avremo un cambio di valore, mentre se sulla stessa riga v'è una linea sola è sinonimo di \textbf{alta impedenza}\footnote{La parte non può agire poiché non riceve segnale}.
\begin{eg}
	\textbf{BUS Sincrono}\par
	\noindent Le operazioni effettuabili sono le classiche lettura e scrittura. La prima vede il Master nel fronte di salita ricevere il dato per poi farlo leggere, produrre ed inviare al BUS Dati dallo Slave nel fronte di discesa.\par
	La seconda invece ha le medesime condizioni iniziali, ma lo Slave agirà prima per scrivere il dato.
\end{eg}
\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\linewidth]{Images/BUS_Sincrono.png}
	\caption{Operazioni in un BUS Sincrono}
	\label{fig:SynBUS}
\end{figure}
\begin{eg}
	\textbf{BUS funzionamento multiciclo}\par
	\noindent Questo utilizzo del BUS sincrono è quello che viene generalmente più utilizzato. Crea un ambiente relativamente solido per effettuare operazioni con la sicurezza di poter gestire ritardi o fallimenti grazie ad un segnale aggiuntivo detto \textbf{Pronto}, il quale si attiva quando l'operazione è stata ultimata. Sceglieremo i cicli minimi che diranno quale coppia Master/Slave è la migliore.
\end{eg}
\begin{figure}[h]
	\centering
	\includegraphics[width=0.6\linewidth]{Images/Funz_Multiciclo.png}
	\caption{Funzionamento BUS Multiciclo}
	\label{fig:SynMulBUS}
\end{figure}
\begin{eg}
	\textbf{BUS Asincrono}\par
	\noindent Qui abbiamo un doppio riscontro detto \textbf{Hand-Shaking} e due segnali \textbf{MasterReady} e \textbf{SlaveReady}. Di per sé non è complesso, ma aiuta molto guardare la figura durante la lettura.\par
	Diciamo di voler effettuare una lettura a tempo generico $i$; per prima cosa il Master dovrà ricevere il segnale del comando, il quale attiverà MasterReady. Ottenuti i dati, li invierà allo Slave che rimanderà tutto al primo come precedentemente visto. Finito di leggere ed una volta confermato che le operazioni sono state completate tramite l'abbassamento di MasterReady, si potrà chiudere l'operazione, liberare il BUS e prepararsi per una prossima istruzione.
\end{eg}
\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\linewidth]{Images/BUS_Asincrono.png}
	\caption{Lettura in un BUS Asincrono}
	\label{fig:AsyBUS}
\end{figure}

%

\section{Stati di un processo}
I concetti di direct memory acces e arbitraggio BUS sono fondamentali in quanto alla base degli elaboratori contemporanei sta il multitasking, che è reso possibile grazie a quanto visto finora. Più nello specifico, andremo a parlare ora di \textbf{Time Sharing}, un meccanismo di condivisione del runtime fra le varie operazioni. Fondamentalmente si divide l'intervallo di tempo reale in vari sottointervalli di egual misura, la cui dimensione è un \textbf{Quanto}, che corrisponde ad $1ms$.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.6\linewidth]{Images/TimeSharing.png}
	\caption{Grafico temporale del Time Sharing}
	\label{fig:TimeSharing}
\end{figure}
Fondamentalmente, comincia il primo processo che ha a disposizione $1Q$ di tempo per lavorare e quando finisce questo arco di tempo, si passa al processo successivo e così via fino al loro termine. Tuttavia, nell'eventualità di una SVC, il processo viene interrotto fino alla comunicazione dell'interrupt.\par
In sostanza, la macchina non sta elaborando più processi allo stesso tempo, bensì uno alla volta ma ad una tale velocità che ciò risulta impercettibile. L'algoritmo è gestito dal \textbf{Kernel} del sistema operativo, il quale vede i progressi di un processo in base allo stato in cui si trovano, ovvero i seguenti:\newline
\begin{minipage}{0.475\linewidth}
	\includegraphics[width=1\linewidth]{Images/ProcFSM.png}
	\label{fig:ProcFSM}
\end{minipage}
\vspace{0.02\linewidth}
\begin{minipage}{0.475\linewidth}
	\begin{itemize}
		\item \textbf{ExecutionSystem}: Dato dalla CPU, è lo stato dove il sistema operativo ha libero accesso.
		\item \textbf{ExecutionUser}: Dato dalla CPU, qui il sistema operativo ha accesso limitato per consentire all'utente di agire in caso di una SVC\footnote{Nell'ISA Intel 80x86 le SVC svolgono anche la funzione di interrupt signals e si indicano con "int".}.
		\item \textbf{Waiting}: Stato dove si spostano i processi non terminati mentre la macchina ne esegue altri.
		\item \textbf{Ready}: Stato dove si trovano i processi interrotti ora pronti per essere eseguiti. Si basa sulla regola LIFO.
	\end{itemize}
\end{minipage}
\newline\noindent Mentre la CPU conta i cicli di clock dedicati agli interrupt signals, il sistema operativo può intervenire sui processi grazie allo \textbf{Scheduler}, il cui compito è decidere se dare ulteriore tempo ad un processo per interromperlo o terminarlo in base al tempo utilizzato in un quanto prima della SVC. Se è meno della metà, il sistema operativo aspetterà il prossimo ciclo di clock, altrimenti lo sposta in E-System. Questo meccanismo è detto \textbf{Preemption}.\par
Se uno scheduler lavora in tempo reale, i processi da lui gestiti si diranno \textbf{corretti} e produrranno un risultato giusto nell'intervallo di tempo giusto. In merito diremo inoltre:
\begin{itemize}
	\item \textbf{Soft RealTime}: Se il processo è stato ultimato sforando di poco l'intervallo di tempo a disposizione.
	\item \textbf{Hard RealTime}: Se il processo è stato ultimato entro l'intervallo di tempo a disposizione.
\end{itemize}
\noindent Ogni singolo processo ha poi un suo descrittore che fa parte di una struttura dati del sistema operativo le cui parti sono: ProcessID, Proprietà, Stato della CPU\footnote{Il salvataggio dello state nella memoria.}, Cache e FileID. Questo tipo di strutture è salvato dallo scheduler e si dice \textbf{Context Switch}. Tuttavia, essendo che necessita del tempo, tutte le istanze in cui esso avviene sono tempi persi. Quanto visto finora crea l'illusione per la macchina di avere una CPU per ogni singolo processo, idem per i dispositivi di I/O.

%

\section{Pila e gestione Interrupt}
Avrai con ogni probabilità sentito parlare del termine \textbf{Stack}. Conoscere il suo funzionamento è \textit{fondamentale} per comprendere appieno come i programmi vengono trattati dalla macchina. Si tratta di una zona di memoria con due caratteristiche:
\begin{itemize}
	\item \textbf{Ristretta}: Vengono selezionati rispettivamente un indirizzo di fine ed uno di inizio per delimitare lo spazio apposito per il programma. I loro valori sono salvati in due registri posti all'inizio e la fine di questa zona di memoria.
	\item \textbf{Riservata}: Ciò è necessario perché se altri processi dovessero accedere ad una zona di memoria già usata, si sfalserebbero o sovrascriverebbero i dati. 
\end{itemize}
\noindent La Pila è divisa propriamente in quattro parti quando questa è ristretta per un programma:\newline
\begin{minipage}{0.475\linewidth}
	\centering
	\includegraphics[width=0.5\linewidth]{Images/Stack.png}
	\label{fig:Stack}
\end{minipage}
\vspace{0.02\linewidth}
\begin{minipage}{0.475\linewidth}
	\begin{itemize}
		\item \textbf{Codice}: Registri per il salvataggio del codice scritto.
		\item \textbf{Dati Statici}: Registri per il salvataggio di costanti simboliche.
		\item \textbf{Dati Dinamici o Heap}: Registri per la memoria temporanea.
		\item \textbf{Ulteriore stack}: Il resto della pila non utilizzato per il nostro programma.
	\end{itemize}
\end{minipage}
\newline\noindent La stack è capace di allocare le variabili locali e passare parametri a funzioni. Proprio grazie a queste ultime è possibile ottimizzare l'utilizzo della memoria, in quanto lo spazio eventualmente creato per queste "vive" fino al loro termine, rendendolo riutilizzabile. Detto ciò, esistono tre azioni \textit{illegali} che portano direttamente ad errori fatali:
\begin{itemize}
	\item Tentativo di accesso a zone di memoria al di fuori dello spazio creato per il dato processo; Se ciò accade, interviene il sistema operativo mediante una routine simil-interrupt per fermare il programma.
	\item Esecuzione di istruzioni che il processore non è abilitato ad effettuare; risulterà in un errore di Interrupt Service Routine.
	\item Lettura di sequenze di bits non integrate nell'ISA, di conseguenza irriconoscibili per la macchina; Avrà luogo una \textbf{TRAP}. Il microprocessore effettuerà una SVC per attivare l'ISR e fermare il processo. L'eventuale report sarà condiviso mediante registri.
\end{itemize}
\noindent Mettiamo di avere un semplice programma in C, abilitato all'utilizzo di una funzione; in tal caso avremo che gli eventuali parametri da essa ricevuti saranno posti in cima alla pila, su registri antecedenti quelli della funzione main. Questo perché la stack è gestita con la regola \textbf{LIFO}, ovvero last-in, first-out.\par
Questo meccanismo è ottenuto posizionando il Program Counter in cima alle celle di memoria allocate per il main, insieme ad una necessaria zona del main dove ritornare il valore elaborato, per evitare che si perda. Infine, terminata la funzione, lo spazio che è stato utilizzato deve essere liberato dall'utente.\par
Anche il sistema operativo ha una propria stack, dove sono presenti PSW e PC per effettuare le routines quando richiesto. Nel momento in cui termina, i registri nominati vengono scaricati.

%

\section{Device driver}
Gli elaboratori odierni hanno una caratteristica comune: necessitano di componenti software che compongono il sistema operativo, e che quindi gestiscano il flusso di input/output. In tal merito, nei dispositivi I/O andiamo a distinguere due livelli:
\begin{itemize}
	\item \textbf{Unità di controllo}, hardware.
	\item \textbf{Device driver}, software.
\end{itemize}
\noindent È grazie al connubio fra queste due parti che possiamo avere un'astrazione tale da consentirci di programmare senza tener conto delle routine a basso livello del sistema operativo. Abbiamo già parlato di come ogni dispositivo I/O utilizzi il meccanismo dell'interrupt; tuttavia questo è legato alla frequenza con la quale avvengono le varie chiamate. Si presenta quindi il problema di gestire più device dalle diverse frequenze.\par
La soluzione ha nome UNIX: rimandare ogni dispositivo I/O al concetto di file, indipendentemente dall'hardware della macchina. Ciò consente di poterci leggere, scrivere, oppure di fare operazioni di controllo input-output. In parole povere, il \textbf{device driver} è un file con un algoritmo.\newline

\noindent Andando ora a parlare più nello specifico dei sistemi operativi, di norma sono costruiti secondo il modello \textbf{onion skin}, dove ogni strato più all'esterno arricchisce quello interno. Si compone di:
\begin{itemize}
	\item \textbf{Hardware}, autoesplicativo.
	\item \textbf{Gestore dei processi}: Consente di gestire più processi grazie al meccanismo di time sharing.
	\item \textbf{Gestore della memoria}: Divide la memoria in tanti sottoinsiemi, ognuno dei quali è dedicato ad un singolo processo. Queste zone di memoria sono inviolabili.
	\item \textbf{Device driver}: Astrae la CPU del dispositivo I/O nel quale è inserita, facendolo vedere come un file. Dà inoltre l'impressione ad ogni processo di avere un dispositivo dedicato.
	\item \textbf{File system}: Gestisce i dispositivi visti sotto forma di files.
	\item \textbf{Gestore rete}: Comunica e gestisce la connessione con la rete internet.
	\item \textbf{Interfaccia utente}, autoesplicativo.
	\item \textbf{Software applicativo}: I programmi.
\end{itemize}
\noindent Dove questo schema rende bene un'idea della struttura dei sistemi operativi, è leggermente datato, perché oggi, per migliori prestazioni, è tutto scritto nel kernel e gestito da appositi moduli:
\begin{figure}[h]
	\centering
	\includegraphics[width=0.95\linewidth]{Images/Kernel.png}
	\caption{Kernel moderno}
	\label{fig:Kernel}
\end{figure}
\newline\noindent Come puoi vedere abbiamo i seguenti moduli del kernel:
\begin{itemize}
	\item \textbf{Hardware Dependent Software}: Algoritmi che lavorano a stretto contatto con l'hardware e che ne sono dipendenti.
	\item \textbf{Gestore della memoria}: Permette che solo lo spazio di memoria scrivibile sia segnato da un processo. Gestisce anche la memoria virtuale.
	\item \textbf{Device Driver}, il quale è diviso in ulteriori blocchi:
	\begin{itemize}
		\item \textbf{DD-Caratteri}: Trasferisce caratteri.
		\item \textbf{DD-Blocchi}: Trasferisce minimo blocchi.
		\item \textbf{DD-File System}: Gestisce il file system.
		\item \textbf{DD-Network}: Trasferisce dati in forma di messaggio, ovvero da un intestatario ad una destinazione.
		\item \textbf{DD-Other}: Usato per ulteriori tipi di dispositivi I/O, come le chiavi USB.
	\end{itemize}
\end{itemize}
\noindent E i relativi servizi del sistema operativo, costruiti in base ai moduli precedenti ed utilizzati con il software applicativo:
\begin{itemize}
	\item \textbf{Device Software}: Gli algoritmi del sistema operativo a livello più alto.
	\item \textbf{Networking}: Gestione della rete.
	\item \textbf{File System}: Gestione dei files.
	\item \textbf{Memory Management}: Gestione della memoria, per esempio come con malloc.
	\item \textbf{Process Management}: Gestisce e consente di creare più processi allo stesso tempo, per esempio come con fork.
\end{itemize}

%

\section{Tipi di Memoria RAM}
Parliamo di \textbf{Random Access Memory}. Si dice tale perché il tempo di accesso ai registri è indipendente dalla distanza percorsa dai segnali. Si compone di varie celle di bits organizzate e distanziate opportunamente entro un certo numero di bit, rendendole \textbf{indirizzabili}\footnote{Per accedere alla memoria è necessario sapere dove essa si trova, quindi conoscere il suo indirizzo.}. Di fondamentale importanza sono le \textbf{WordLines} e le \textbf{BitLines}, le quali consentono di individuare un singolo bit di una singola parola. Le linee sono comuni ad ogni tipo di RAM, ma ne vedremo solo i primi due principali: \textbf{static RAM} e \textbf{dynamic RAM}.\newline

\noindent Cominciamo dal primo tipo; la memoria ad accesso casuale statica è generalmente più prestante, nonostante richieda più transistor rispetto alla seconda. Presenta una wordLine orizzontale e due bitLine verticali, queste ultime collegate ad un circuito chiamato \textbf{Level Sense}, che è capace di scegliere l'operazione read/write in base ad un segnale dato dal bus controllo\footnote{Se riceve valore $0$, scrive, se $1$, legge.}. Il risultato è poi spedito al bus dati.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.6\linewidth]{Images/SRAM_min.png}
	\caption{Singola cella di S-RAM}
	\label{fig:SRAM_min}
\end{figure}
\newline\noindent Se la singola parola è selezionata, la wordLine avrà valore $1$, in alternativa sarà falsa; inoltre, è possibile modificare la parola e non il singolo bit specifico. Per essere precisi, avviene:
\begin{itemize}
	\item Se si vuole memorizzare $1$, sarà presente nella bitLine di sinistra, con $0$ nella destra.
	\item Se si vuole memorizzare $0$, sarà presente nella bitLine di sinistra, con $1$ nella destra.
\end{itemize}
\noindent Notare che le parole sono composte da $32b$, quindi, il circuito Level Sense è collegato ad uno specifico bit del bus dati; di conseguenza ci saranno tanti circuiti minimi di S-RAM quanti sono i bit del bus dati. In questo caso, $32$. Se poi level sense decide di leggere, guarda alla bitLine sinistra e capisce la parte destra in base al valore della prima. Metterà poi il valore sul bus dati. Se scrive, invece, prende il bit dal bus dati e lo riporta in output.\par
Proviamo adesso ad estendere il concetto e gestirlo su più bit. Consegue capire che la RAM è un vettore di parole. Supponiamo ora di avere una matrice $1024b\times32b$, quindi $1024$ parole e righe totali con $32$ colonne di bits.\par
Naturalmente, i level sense devono essere collegati a bus controllo e bus dati, ma abbiamo ora un'aggiunta direttamente dal bus indirizzi: \textbf{ChipSelect}. Se vale $1$, farà lavorare il level sense scelto, disattivando gli altri.\par
Per trovare il totale dei bit del bus indirizzi è necessario usare le potenze del $2$ per trovare il numero che corrisponde alle righe. Nel nostro caso, $2^{10} = 1024$, quindi avremo $10b$ di indirizzi. Questi sono poi collegati tramite un decoder che andrà a generare le nostre $1024$ wordlines. Selezionando infine un singolo bit dei $1024$ si arriverà la singola riga e tutte le altre saranno a $0$.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\linewidth]{Images/SRAM.png}
	\caption{Esempio di S-RAM}
	\label{fig:SRAM}
\end{figure}
Espandiamo le dimensioni e diamoci il compito di creare una matrice di dimensione $1Mb\times 64b$. Avremo:
\begin{itemize}
	\item Totale bits bus indirizzi: $1Mb = 2^20 \implies 20b$
	\item Totale wordLines: $1Mb = 1024Kb$.
\end{itemize}
\noindent Iniziamo dicendo che le parole sono fatte da massimo $32b$, quindi per gestirne $64$ avremo bisogno di più coppie di circuiti S-RAM. Saranno necessarie coppie di circuiti che abbiamo creato prima; il primo elemento gestirà i bit più significativi, mentre il secondo si occuperà degli altri.\par
Quindi per selezionare il bit corretto si attiveranno le wordLines dei decoder se è necessario un bit meno significativo [0-9], in alternativa verrà usato chipSelect, uscente da un altro decoder supplementare, per prendere i bit più significativi [10-19].\newline
% TODO INSERISCI IMMAGINE S-RAM COMPLESSA
% DRAM

\noindent Nella \textbf{D-RAM} non è possibile memorizzare indefinitamente il contenuto di una cella poiché la memoria effettua un \textbf{refresh} periodico che libera lo spazio, indipendentemente dal fatto che una cella sia piena o meno. Questa memoria si compone da una sola wordLine e una bitLine. La cella da $1b$ presenta un transistor collegato ad un condensatore, un accumulatore di carica elettrica costante rispetto alla propria capacità; al suo interno si trova dell'isolante elettrico, il quale rilascia il segnale ad ogni refresh.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.6\linewidth]{Images/DRAM.png}
	\caption{Singola cella di Dynamic RAM}
	\label{fig:D-RAM}
\end{figure}
\newline\noindent La D-RAM risulta più lenta della S-RAM perché si scarica anche se isolata e di conseguenza, in caso di necessità, deve essere nuovamente riempita. Anche la sola operazione di lettura è distruttiva, perché se si vuole accedere al contenuto è necessario liberarlo. Essendo poi una memoria più compatta e capiente, è molto probabile interfacciarsi con dimensioni di bit molto grandi con di conseguenza molti più segnali.

%

\section{Caratteristiche e gerarchia delle memorie}

% TODO QUI VA BENE LO SPEZZONE DELLO SCORSO ANNO

%

\section{Memoria Cache, Paginata e Virtuale}
La \textbf{memoria cache} è una componente installata direttamente nel microprocessore, non visibile al software e completamente gestita dall'hardware. Il suo scopo è la memorizzazione dei dati più recentemente usati dalla memoria principale ed è capace di velocizzarne gli accessi, aumentando le prestazioni del sistema. Nello specifico, quando un dato si trova all'interno di questa memoria, è preso in un solo ciclo di clock.\par
La memoria cache è relativamente più piccola della ram; ne consegue che non è possibile accedere ad ogni dato a velocità istantanea. Definiamo infatti:
\begin{itemize}
	\item \textbf{Cache hit}: Quando il dato richiesto è attualmente salvato nella memoria cache.
	\item \textbf{Cache miss}: Quando il dato richiesto non si trova nella cache e deve essere pescato.
\end{itemize}
\noindent Prima di parlare dell'algoritmo di spostamento dati da ram a cache, è necessario chiarire alcune dinamiche e compiere delle operazioni preliminari per assicurarsi delle dimensioni delle componenti prese in esame. Inoltre, le memorie cache e ram non trasferiscono una singola parola, bensì copieranno un'intera pagina, che è un insieme di queste ultime. Ciò avviene poiché per il principio di località, gli accessi successivi si trovano nelle parole successive presenti nella stessa pagina, velocizzando le operazioni. Supponiamo ora di avere $4GB$ di ram e $4MB$ di cache, con pagine grandi $1KB$, allora:
\begin{itemize}
	\item Dimensione indirizzi ram: $4GB = 2^2\times 2^{30} = 2^{32} \to 32b$.
	\item Dimensione indirizzi cache: $4MB = 2^2\times 2^{20} = 2^{22} \to 22b$.
	\item Dimensione pagine: $1KB = 1024b = 2^{10} \to 10b$.
	\item Divisione in pagine della ram: $\frac{4GB}{1KB} = 4M = 2^2\times 2^{20} \to 22b$.
	\item Divisione in pagine della cache: $\frac{4MB}{1KB} = 4K = 2^2\times 2^{10} \to 12b$.
\end{itemize}
\noindent Quindi abbiamo già capito la struttura degli indirizzi sia per ram che per cache. Nella prima si necessitano $22b$ per differenziare ogni pagina, mentre per la seconda, $12b$. I bit restanti saranno usati per definire la parola.\par
Questo era un procedimento generale, tuttavia, l'algoritmo di indirizzamento presenta tre tipi diversi:
\begin{itemize}
	\item \textbf{Diretto}: Necessita di uno spazio apposito dedito al vettore delle etichette, grande quanto il numero di pagine della cache.\par
	Si pone il compito di dare ad ogni singola pagina della ram una sola posizione in cui andare nelle pagine cache. Si rende quindi necessario dividere i bit della ram in modo diverso:
	\begin{center}
		$10b$ per la parola, $12b$ per la pagina e $10b$ per l'etichetta della pagina.
	\end{center}
	% TODO RIVEDERE QUESTA PARTE (DIRETTO)
	\item \textbf{Associativo}: Con questo algoritmo non siamo certi di dove vada una specifica pagina, perché verranno controllate tutte fin quando non si troverà quella corrispondente, come un ciclo for che fa break solo quando l'indice è arrivato ad un certo valore.\par
	Qui vengono tenuti i $10b$ per la parola, mentre i restanti $22b$ della ram sono usati per l'etichetta. Una volta trovata, si darà l'indirizzo in cache.
	\item \textbf{Set-Associativo}: Sinergia fra i due metodi precedenti. Ci è possibile raggruppare delle pagine in insiemi per poter effettuare il controllo delle etichette direttamente in tal gruppo.\par
	Nella divisione dei bits dell'indirizzo in ram, oltre ai $10b$ per la parola, al posto della pagina avremo, dipendentemente da quanti insiemi sono presenti in memoria, un numero $n$ di bits per i set, mentre per l'etichetta saranno usati i rimanenti.
\end{itemize}
\noindent Questo è il funzionamento in condizioni ideali, ma ora bisogna considerare l'eventualità di avere una memoria cache piena e di aver effettuato un cache miss. Con che modalità si sceglie quali dati tenere e quali rimuovere per fare spazio agli altri? Questo dipende dallo scopo della macchina, ma esistono due \textbf{algoritmi di sostituzione} principali e riguardano rispettivamente lo scarico della pagina più vecchia, \textbf{LRU}, Least Recently Used, o la più recente, \textbf{MRU}, Most Recently Used.\par
Questi algoritmi funzionano grazie alla presenza di un contatore in ogni pagina, incrementato ad ogni ciclo di clock.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.525\linewidth]{Images/addressingBits.png}
	\caption{Divisione in bit per i metodi di indirizzamento}
	\label{fig:addrBits}
\end{figure}
\newline\noindent Negli elaboratori odierni, la memoria centrale fisica non è grande quanto lo spazio di indirizzamento gestibile da parte del processore, ragion per cui, come fatto nelle sezioni precedenti, è necessario dare alla macchina l'impressione di avere a disposizione tutta la memoria per ogni singolo programma; parliamo quindi di \textbf{memoria virtuale}. Per questo scopo, introduciamo gli algoritmi di:
\begin{itemize}
	\item \textbf{Rilocazione}: Strategia che permette di usare la stessa RAM senza sovrascrivere i registri.
	\item \textbf{Paginazione}: Strategia che permette a più processi di utilizzare efficientemente la stessa RAM.
\end{itemize}
\noindent Partiamo da un semplice presupposto; il processore, per poter lavorare con dei dati o prelevare un'istruzione, genera un indirizzo binario chiamato \textbf{indirizzo virtuale}, il quale sarà tradotto in \textbf{indirizzo fisico} tramite un algoritmo svolto da sistema operativo ed unità funzionali.\par
Se l'indirizzo logico fa riferimento ad una parte di spazio di codice in memoria primaria, si otterrà accesso immediato, in alternativa dovrà essere ripescato dal disco fisso e posizionato in cache tramite apposito algoritmo di sostituzione. Il processo di traduzione degli indirizzi è effettuato mediante la \textbf{rilocazione}, per poi inviare il risultato al bus indirizzi. Ci sono due modalità:
\begin{itemize}
	\item \textbf{Rilocazione statica}, usata per i sistemi embedded.\par
	\noindent Supponiamo che un programma abbia inizio all'indirizzo $1438$; a questo numero saranno sommati tutti gli indirizzi logici dei dati del codice, così da non sovrascrivere i registri precedenti. Eventualmente, se sono presenti cicli o costrutti condizionali, il codice si biforcherà dipendentemente dalla loro quantità. Ripetere fino a completa scrittura del programma.
	\item \textbf{Rilocazione dinamica}, apposita per i dispositivi programmabili.\par
	\noindent Utilizza due registri, \textbf{base} e \textbf{limit}, i quali contengono rispettivamente la prima e l'ultima zona di memoria creata per il programma. Questi lavorano insieme ad un circuito che controlla se la somma fra indirizzo logico e base è compresa nella zona di memoria apposita. Se lo è, crea l'indirizzo fisico e lo invia al MAR, altrimenti si ha un errore di Segmentation Fault.
\end{itemize}
\noindent C'è tuttavia un problema in questi metodi. Supponiamo di voler caricare tre diversi programmi: $P_1$, $P_2$, $P_3$, i quali verranno posizionati in ordine nelle rispettive zone di memoria.\par
$P_1$ richiede una malloc, ma non è possibile donare altro spazio poiché la zona ad esso successiva è usata da $P_2$ e non è possibile spostare i processi, oppure mettiamo caso che $P_2$ finisca il suo lavoro e devi inserire $P_4$ ma non è presente spazio sufficiente per contenerlo tutto. Ne consegue che è necessario ideare un algoritmo per la gestione dello spazio: la \textbf{paginazione}. Si tratta dell'assegnazione ad ogni processo di un numero $n$ pagine indipendentemente dalla loro posizione fisica. Ciò risolve il problema alla radice perché consente di usare zone di memoria non necessariamente adiacenti.\par
Ma come funziona? Abbiamo la solita divisione dei $32b$ nella memoria RAM, dove $10b$ rappresentano la parola e i restanti $22b$ l'indirizzo logico che poi verrà tradotto in indirizzo fisico. Noi potremmo assegnare a questi bit di indirizzo una pagina logica e una fisica, mantenendo lo stesso procedimento visto nella scorsa sezione. Ciò avviene grazie alla presenza della \textbf{Tabella delle pagine}; una matrice tanto grande quanto le pagine del processo, dove sono scritte in ordine crescente tutte le pagine logiche insieme alla loro rispettiva pagina fisica.\par
Questa matrice è contenuta in un circuito chiamato \textbf{MMU}; Memory Management Unit, presente nella CPU e guidato dal sistema operativo. Riceve i dati dalla tabella con la possibilità di ampliarla quando richiesto. Può inoltre riconoscere se le informazioni ricevute sono utili o meno, ed eventualmente scartarle. Questa unità gestionale è necessaria per due motivi: non appesantire il sistema operativo e sfruttare il principio di località a proprio vantaggio, migliorando le prestazioni. Tuttavia, quanto è grande la tabella delle pagine? Comprenderà certamente l'insieme delle pagine necessarie per il funzionamento del programma, detto \textbf{Working Set}.\par
Ogni processo avrà il proprio working set, ma una cosa meno ovvia è come si tratta del giusto equilibrio fra tempo di esecuzione ed il numero di pagine minimo utilizzabile, allo scopo di garantire le prestazioni migliori.\newline

\noindent Arriviamo al dunque; come viene creata ed utilizzata la memoria virtuale? Partiamo dal working set. Come precedentemente menzionato, ogni processo ha il proprio che lavora in solitaria e bisogna convincere il sistema operativo che ha a disposizione tutta la RAM per ogni processo.\par
Si prende quindi una sezione di memoria su disco, detto \textbf{Swap} e donarlo al sistema operativo. Se una pagina del set viene usata poco\footnote{Questo è un controllo eseguito mediante contatori nella tabella delle pagine. Sono incrementati ad ogni ciclo di clock.}, verrà spostata in questo swap per liberare spazio in RAM. Notare che l'algoritmo non butta via le pagine perché sempre nella tabella è presente un'area apposita che dice se una pagina si trova o meno nello swap.\par
Nel caso in cui serva una pagina in esso presente, sovrascriverà sempre la pagina più vecchia in ram. È necessario trovare anche qui un compromesso, poiché se allo swap è dato troppo spazio, potrebbero risultare svariati problemi nell'esecuzione dei programmi.

% TODO RIFAI IMMAGINE RELOCATION E WSETGRAPH

\begin{figure}[h]
	\centering
	\includegraphics[width=1\linewidth]{Images/wordToPage.png}
	\caption{Percorso di paginazione}
	\label{fig:wrdToPage}
\end{figure}

%

\section{Pipelining}
Finora abbiamo considerato la processione dei dati come una FSM a quattro stati, ma è possibile dividere l'ultimo stato di execution fra \textbf{esecuzione istruzione} e \textbf{salvataggio risultato}, chiamando quest'ultimo \textbf{WriteBack}.\par
Immaginiamo ora di avere un circuito semplice di somma, dal leggero costo in runtime e con un'architettura ottimizzata con un circuito esterno e registri flip-flop. Avremo:
\begin{enumerate}
	\item \textbf{Fetch}: Ricezione istruzioni in $1CPI$.
	\item \textbf{Decode}: Decodifica istruzioni con eventuali accessi in memoria, dipendentemente dalla loro necessità si useranno $0CPI$ se non acceduto, $1CPI$ altrimenti.
	\item \textbf{Execution}: Esecuzione istruzioni in $1CPI$.
	\item \textbf{WriteBack}: Salvataggio risultato quando richiesto. Usa $1CPI$.
\end{enumerate}
\begin{table}[h]
	\centering
	\begin{tabular}{|c|c|c|}
		\hline
		& Ciclo di Memoria & Ciclo di clock\\
		\hline
		Fetch & 1 & 1\\
		\hline
		Decode & da 0 a 1 & 1\\
		\hline
		Execution & 0 & 1\\
		\hline
		WriteBack & da 0 a 1 & 1\\
		\hline
		Totale & da 1 a 2 & 4\\
		\hline
	\end{tabular}
	\caption{Costo in accessi a memoria/cicli di clock dei quattro stati}
	\label{Tabella}
\end{table}
\noindent Puoi osservare nella tabella che il numero minimo di cicli ottenibile è $1 + 4 = 5$, $6$ nel peggiore dei casi. Questo è ottenibile solamente con un ISA richiedente un solo accesso a memoria, ma possiamo ottimizzare ulteriormente.\par
È possibile minimizzare la memoria scegliendo quale operazione fra decode e writeBack avrà il diritto di accedervi. Conviene inoltre dividere i bits della memoria cache in due parti: una per i dati e l'altra per le istruzioni, rendendo possibile farle lavorare in contemporanea a due mansioni diverse.\par
Possiamo andare oltre. Introduciamo quindi la strategia di \textbf{pipelining}; la quale vede i quattro stati messi in fila come una catena di montaggio. Questi sono separati da dei banchi di registri che salveranno quanto svolto per darlo allo stato seguente.\par
Questo sistema consente alle quattro istruzioni di lavorare indipendentemente l'una dall'altra\footnote{Nella maggior parte dei casi è così. L'eccezione è quando si hanno cicli e condizioni.}, riducendo drasticamente i cicli di clock necessari per elaborare un programma. Generalmente, segue questi passi:
\begin{enumerate}
	\item \textbf{Fetch} prende dai registri il valore del PC e dalla cache istruzioni il compito che dovrà eseguire. Incrementerà di uno il PC e passerà nel banco il valore di quest'ultimo insieme a quello dell'IR.
	\item \textbf{Decode} riceve le informazioni dal banco precedente e, dipendentemente dalla modalità di indirizzamento, prenderà il codice dalla cache o dai registri. Metterà infine nel suo banco gli operandi ed il valore del PSW.
	\item \textbf{Execution} riceverà gli operandi e ne calcolerà il risultato. Aggiornerà infine i registri e metterà il tutto nel suo banco insieme al PSW aggiornato.
	\item \textbf{WriteBack} infine salverà questi risultati nella cache dati o nei registri, dipendentemente da dove è richiesto.
\end{enumerate}
\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\linewidth]{Images/pipeline.png}
	\caption{Circuito di pipeline}
	\label{fig:Pipeline}
\end{figure}
Nella teoria si svolge in questo modo, tuttavia passando al lato pratico si presentano un paio di problemi. Il primo è dato da un'eventuale complessità degli stati ed un conseguente sfalsarsi delle operazioni col clock. Un secondo problema è dato dalle \textbf{bolle}, ovvero istruzioni che non possono essere eseguite nello stesso ciclo di clock a causa di una dipendenza condizionale o ciclica.\par
Osserva bene la tabella seguente considerando la singola funzione di ogni istruzione. Al quarto ciclo di clock si vuole sottrarre il valore 1 al contenuto del registro \%EBX, ma questo non è stato ancora preparato da WriteBack, quindi sarà necessario aspettare un ciclo di clock per effettuare poi il decoding. Da questo puoi notare come un registro ritorni pronto solo dopo l'esecuzione della WriteBack.\par
Esistono inoltre delle strategie per la riduzione delle bolle; per le condizioni si allontanano le istruzioni che dipendono le une dalle altre, mentre per i cicli ci si affida ad un riconoscimento di pattern tramite le \textbf{jump predictions}\footnote{La CPU osserva il primo risultato della condizione e implica che questo avvenga anche alla prossima richiesta. Si prepara di conseguenza per l'output che si aspetta.}.\par
In tal caso, se il risultato del salto è predetto correttamente, si potrà effettuare il fetch della prima istruzione al nono ciclo di clock, ottenendo un CPI medio di $\frac{12-4}{6} = 1,35CPI$, che si avvicina molto al nostro obiettivo.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\linewidth]{Images/pipelineInstr.png}
	\caption{Istruzioni pipeline}
	\label{fig:pipelineInstr}
\end{figure}

%

\section{Modello CISC e RISC}
Parliamo adesso di com'è nato il modello di architettura sul quale stai leggendo questo PDF: il \textbf{RISC}, Reduced Instruction Set Computer; figlio del connubio di tutto ciò che abbiamo studiato finora. Si tratta dell'architettura con l'attuale migliore ottimizzazione del tempo impiegato dalla CPU\footnote{Più precisamente, vale l'equazione: $T_{CPU} = Tot_{Istruzioni} \times CPI_{Medio} \times \frac{1}{Freq_{Clock}}$.} per elaborare le informazioni.\par
Giusto neanche cinquant'anni fa, il mondo ha visto una grandissima opportunità di mercato nei microprocessori e, di conseguenza, molte aziende come Intel hanno provato a prendere parte alla corsa per fare i grossi soldi. L'intenzione è creare il microprocessore più efficiente possibile per sbaragliare la concorrenza. Come fare? La prima idea diffusa su vasta scala fu quella di aumentare la frequenza dei cicli di clock. Essendo che la frequenza del clock è data dal Datapath, il quale ha un cammino critico, si pensò di utilizzare delle pipelines per ottimizzare le istruzioni dell'ISA.\par
Fatto sta che il funzionamento della pipeline dipende dalla quantità ed il tipo di istruzioni presenti nell'insieme e siamo inoltre limitati dalla potenza delle componenti fisiche. Agiremo, dunque, sul valore totale delle istruzioni nell'ISA, modificandole e rendendole molto più complesse e compatte. Abbiamo ottenuto un valore molto minore di direttive, ma avremo come conseguenza un datapath che rispecchia la complessità di quanto rielaborato e quindi un cammino critico molto più impegnativo, per non parlare della pipeline che deve costantemente aspettare il lavoro finito dello stato precedente.\newline

\noindent Qui entrarono in gioco due professori dell'Università di Stanford; creando l'architettura sulla quale si basano gli elaboratori contemporanei: \textbf{John Hennessy} ed il suo collaboratore \textbf{David Patterson}.\par
"E se fossero le istruzioni a dover essere ridotte per ottenere prestazioni migliori?" Fu la fatidica domanda che smosse il tutto. Chiesero ad altri ricercatori di creare dei programmi in diversi linguaggi e notarono una particolarità fra tutti i codici: erano scritti in modo semplice,  indipendentemente dalla complessità del linguaggio. Come diretta conseguenza, lo erano anche le istruzioni in codice Assembly.\par
Con questa filosofia i due riuscirono a creare il DLX, il primo vero dispositivo RISC e con i dati alla mano, nonostante i soliti dubbi dal mercato, riuscirono a dimostrare di aver creato un microprocessore nettamente migliore a quelli delle altre aziende, che decisero di chiamare \textbf{CISC}, Complex Instruction Set Computer. I dispositivi RISC, oltre ad avere poche istruzioni, hanno il vantaggio di avere pochi metodi di indirizzamento. Questa semplicità aiuta il funzionamento della pipeline, la quale sarà nettamente più veloce, ottenendo non solo un numero molto minore di CPI medio, ma anche una frequenza di clock maggiore rispetto alle altre architetture.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\linewidth]{Images/HenPat.jpg}
	\caption{John Hennessy e David Patterson}
	\label{fig:twoGOATS}
\end{figure}
Annusata l'opportunità per fare soldi, le aziende iniziarono a cimentarsi nella filosofia RISC, ottenendo risultati sulla potenza in pochissimo tempo:
\begin{itemize}
	\item \textbf{Intel} fu come al solito la prima e creò il MIPS, con una frequenza di 100MHz.
	\item \textbf{Motorola} seguì subito con la creazione di PowerPC, da $200MHz$.
	\item \textbf{DEC} poi creò Alpha dai sorprendenti $600MHz$.
	\item Il \textbf{Stanford University Network} infine creò la SPARC su FPGA, da $300MHz$.
\end{itemize}
\noindent Nella foga per l'ottimizzazione, gli ingegneri a Intel crearono il microprocessore P6, che per quanto ottimale potesse risultare al tempo, aveva un ISA completamente diverso da quello utilizzato da loro finora (e tuttora); Intel 80x86.\par
Si dovette creare un chip più grande con lo scopo di, mediante una fase di \textbf{Pre-Fetch}, tradurre l'ISA diverso nell'80x86. Chiamarono questa architettura \textbf{Pentium}. Attualmente nessuno di questi microprocessori è più in uso, ma immagino tu già lo sappia, dato che abbiamo ben superato il GigaHz di frequenza.

%

\section{Architetture parallele}
È nostro volere ottimizzare quanto possibile, quindi bisogna capire cosa vuol dire veramente migliorare le prestazioni di un calcolatore.\par
Da un punto di vista dell'utente medio si può pensare a ridurre il tempo totale di esecuzione della CPU, che non è necessariamente sbagliato, ma è un ragionamento superficiale. Si ritiene invece più importante aumentare il numero di processi eseguibili in un determinato arco di tempo ed è questo ragionamento che ha portato allo sviluppo delle \textbf{Architetture Superscalari}, modelli con la capacità di elaborare istruzioni con un CPI medio minore di 1, ma come raggiungere questo risultato?\par
Il primo microprocessore vincente fu il \textbf{PowerPC} di Motorola, il quale utilizza una componente di pre-fetch che invia poi i dati da elaborare a due pipelines da quattro stati l'una. La prima avrebbe svolto operazioni solamente con interi, l'altra con numeri in virgola mobile.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\linewidth]{Images/PowerPC Pipelines.png}
	\caption{Funzionamento dual-pipeline di PowerPC}
	\label{fig:PowerPCPIPE}
\end{figure}
Queste due pipelines funzionano in parallelo, quindi dove normalmente avremmo un $CPI = 2$, data la "doppia velocità", abbiamo il $CPI = 0,5$. Inoltre abbiamo la possibilità di donare loro un clock proprio per gestire al meglio le operazioni ed aumentare conseguentemente ancora di più la frequenza di clock. Se far ciò migliora così tanto le prestazioni, perché non inserire sempre più pipelines che lavorano parallelamente, allora?\par
Ebbene, questo è a causa della dipendenza delle istruzioni. Ulteriori pipelines equivalgono a molte condizioni e più ce ne sono, più (se necessario, ma di norma lo è) bisognerà attendere la fine della fase precedente, rallentando l'intero processo e peggiorando le prestazioni. Come risolvere?\par
Un primo tentativo di risoluzione si ebbe con le architetture di microprocessori \textbf{VLIW}, Very Long Instruction Word, il cui scopo è ottenere un livello di astrazione maggiore per le istruzioni, le quali saranno quindi composte di più operazioni, aumentando la dimensione dei banchi di memoria presenti fra la pipeline.\par
Di una diversa linea di pensiero furono i creatori dei \textbf{Calcolatori Vettoriali}; i quali pensarono di utilizzare, insieme alla componente di pre-Fetch e ad un banco di memoria, molte più CPU che lavorano in parallelo. Per esempio, se bisogna eseguire $1000$ somme, si utilizzerebbero $1000$ cicli di clock, ma inserendo $500$ circuiti di somma, ne verranno usati solamente due; un miglioramento spaventoso. Tuttavia, per collegare le mini-CPU è necessario utilizzare una \textbf{matrice di interconnessione}, che causa una sovrabbondanza di collegamenti.\par
Un altro problema è presentato dalla \textbf{Legge di Amdahl}, la quale afferma che "\textit{Il miglioramento delle prestazioni di un sistema che si può ottenere ottimizzando una certa parte del sistema è limitato dalla frazione di tempo in cui tale parte è effettivamente utilizzata}".\par
In merito introduciamo il concetto di \textbf{Speed-up}, che ha origine proprio grazie a questa legge. Si tratta in genere di un valore che misura le prestazioni di un elaboratore in funzione di un determinato programma; in particolare ci interessa la \textit{Latenza}\footnote{$L = \frac{T}{W}$, dove $T$ è il tempo e $W$ il totale del lavoro eseguito per la task.} per poter eventualmente paragonare i tempi di due architetture effettuando un rapporto fra di loro.\par
Visto questo, immagina un calcolatore vettoriale che cerca di svolgere qualsiasi compito diverso da operazioni aritmetiche. Non sembra molto efficiente.\newline

Ipotizziamo ora di non volere un parallelismo per ogni singolo programma, bensì di lavorare su un parallelismo fra i processi e quindi avere più CPU in una singola architettura. Il problema fondamentale di questa idea è che ognuna di loro necessita la propria cache e nell'elaborazione dei dati avremo delle copie esatte di quanto elaborato in ogni CPU, quindi un problema di \textbf{Coerenza di cache}.\par
Per la gestione di questo problema vennero ideati gli \textbf{Algoritmi di Snooping}, che vedono la presenza in ogni microprocessore di una tabella contenente le posizioni di $n$ dati, dopo che questi sono passati per il BUS. Non sono tuttavia soluzioni perfette, poiché il troppo snooping peggiora le prestazioni, senza contare che abbiamo un singolo BUS dove passano le informazioni\footnote{Se non ti sembra aver senso invito la revisione della sezione dedicata all'arbitraggio del BUS.}.\newline

Poi venne il nuovo millennio e cambiò tutto con la tecnologia \textbf{Network On Chip}; la realizzazione di una matrice di interconnessione sul silicio stesso, che consentì di inserire più CPU in una singola architettura. Questi processori vengono chiamati \textbf{Cores} e questo miracolo diede vita ai processori \textbf{Multicore}, inseriti nei dispositivi come quello che stai utilizzando ora. Il grande vantaggio è avere una cache consistente poiché le connessioni della matrice annichiliscono le attese per accedervi.\par
Ma non è finita qui; venne data nuova vita ai calcolatori vettoriali per la necessità della rapida elaborazione delle grafiche a video. Si tratta di un compito particolarmente complesso basato su calcoli computazionali; il punto di forza di tali architetture. Oggi sono conosciuti sotto il nome di GPU, Graphic Processing Unit, ed il capo supremo del loro mercato è NVidia.\par
Col passare del tempo questi modelli si svilupparono e videro la luce le \textbf{GPGPU}, General Purpose GPU, utilizzate per richieste grafiche particolarmente esigenti, come un qualunque gioco PS5 realistico. Si presta inoltre molto bene per la creazione di reti neurali, quindi per la creazione e lo sviluppo di intelligenze artificiali.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.45\linewidth]{Images/ModernArchitecture.png}
	\caption{Architettura di Von Neumann contemporanea}
	\label{fig:modernArch}
\end{figure}