\section{Elementi di probabilità}
La probabilità è una branca della matematica che si occupa dello studio e descrizione degli \textbf{esperimenti aleatori}, ovvero delle inferenze il cui esito non è del tutto prevedibile. Esistono due metodi per l'espressione del concetto di probabilità:
\begin{itemize}
	\item \textbf{Approccio frequentista}; Determinazione della probabilità mediante esperimenti ripetuti. Risulta quindi come il rapporto fra il totale in cui si è esperito un esito e il totale degli esperimenti.
	\item \textbf{Approccio soggettivista}; Dove la probabilità è vista come un livello di fiducia nel verificarsi di un dato esito. È na roba da filosofi, non fa per noi.
\end{itemize}
Abbiamo parlato di una totalità di esperimenti; questi vengono formalmente chiamati \textbf{eventi} $E$ e detengono informazioni riguardo al loro esito. Ogni evento è un sottoinsieme dello \textbf{spazio campionario} $S$, che li comprende tutti.
\begin{eg}
	\textbf{Spazio campionario}\newline
	Un esempio di spazio campionario è dato dalla totalità dei valori delle facce di un dado, mentre gli eventi sono i singoli valori usciti da un esperimento.
	\begin{center}
		$S = \{1, 2, 3, 4, 5, 6\}$, $E = \{4\}$
	\end{center}
\end{eg}
Alle operazioni logiche sulle affermazioni corrispondono quelle insiemistiche,le quali si mostrano mediante diagrammi di eulero venn. Siano $A, B \subseteqq S$ due eventi:
\begin{itemize}
	\item \textbf{Intersezione}\newline
	Quando "Avviene $A$ e avviene $B$"
	% TODO ----- AGGIUNGI DIAGRAMMA
	\item \textbf{Unione}\newline
	Quando "Avviene $A$ oppure $B$"
	% TODO ----- AGGIUNGI DIAGRAMMA
	\item \textbf{Sottrazione}\newline
	Quando "Avviene $A$, ma non $B$"
	% TODO ----- AGGIUNGI DIAGRAMMA
	\item \textbf{Complementare}\newline
	Quando "Non avviene $A$"
\end{itemize}
Inoltre, se gli insiemi $A,B$ sono tali che la loro intersezione sia vuota, si dicono \textbf{incompatibili}.

%

\section{Calcolo della probabilità}
Fortunatamente esiste una concezione standard sulle caratteristiche assunte dalla probabilità. Associamo infatti ad ogni evento $E$ sullo spazio campionario $S$, un valore denotato con $P(E)$, detto \textbf{probabilità dell'evento} $E$. Il comportamento della funzione è dato dai seguenti \textbf{assiomi di Kolmogorov}:
\begin{definition}
	\textbf{Assiomi di Kolmogorov}\newline
	\begin{enumerate}
		\item $P(A)$ è un valore compreso fra $0$ e $1$.
		\item $P(S) = 1$.
		\item Se $A$ e $B$ sono incompatibili, allora $P(A \cup B) = P(A) + P(B)$.
		\item Siano $A, B$ due eventi tali che $A \subseteqq B$, allora:
		\begin{center}
			$\begin{cases}
				B = S \implies S / A = A^c \implies P(A^c) = 1 - P(A)\\
				P(B / A) = P(B) - P(A)
			\end{cases}$
		\end{center}
		\item Se $A_1, A_2, ..., A_k$ sono eventi a due a due incompatibili, quindi disgiunti, allora:
		\begin{center}
			\[P\left(\bigcup_{i = 1}^k A_i\right) = \sum_{i = 1}^{k} P(A_i)\]
		\end{center}
		\item Siano $A, B$ due eventi generici, allora:
		\begin{center}
			$P(A \cup B) = P(A) + P(B) - P(A \cap B)$
		\end{center}
	\end{enumerate}
\end{definition}
Un primo caso di studio per la probabilità è il suo calcolo ad \textbf{esiti equiprobabili}; ciò significa che ogni evento ha la stessa chance di avvenire rispetto agli altri. L'esempio classico è il lancio di un dado; implicando che questo non sia truccato, ogni faccia ha $\frac{1}{6}$ di possibilità di uscire. Formalmente la definiamo con la seguente scrittura:
\begin{center}
	$P(A) = \dfrac{|A|}{|S|}$
\end{center}
\begin{eg}
	Quali sono le probabilità che lanciando due volte un dado esca il valore 7?\newline
	Innanzitutto dobbiamo chiederci quale sia lo spazio campionario e gli eventi. Sappiamo che è un dado, quindi avremo rispettivamente:
	\begin{itemize}
		\item $S = \{1, 2, 3, 4, 5, 6\}$
		\item $E_1 = \{1\}, ..., E_6 = \{6\}$
	\end{itemize}
	Ora, potremmo fare bruteforcing facendoci del male, ma il trucco per questi esercizi (entro certi limiti) è disegnare una tabella dei risultati, prendere il totale di quante volte si presenta il valore richiesto e poi applicare la formula dell'approccio frequentista. In questo caso:
	\begin{center}
		\begin{tabular}{|c||c|c|c|c|c|c|}
			\hline
			- & 1 & 2 & 3 & 4 & 5 & 6\\
			\hline
			\hline
			1 & 2 & 3 & 4 & 5 & 6 & 7\\
			\hline
			2 & 3 & 4 & 5 & 6 & 7 & 8\\
			\hline
			3 & 4 & 5 & 6 & 7 & 8 & 9\\
			\hline
			4 & 5 & 6 & 7 & 8 & 9 & 10\\
			\hline
			5 & 6 & 7 & 8 & 9 & 10 & 11\\
			\hline
			6 & 7 & 8 & 9 & 10 & 11 & 12\\
			\hline
		\end{tabular}
	\end{center}
	Notiamo che il valore $7$ compare $6$ volte ed il totale degli esiti ottenibili è $6 \times 6 = 36$. Il risultato sarà dato quindi da:
	\begin{center}
		$\dfrac{6}{36} = \dfrac{1}{6}$, soluzione dell'esercizio.
	\end{center}
\end{eg}
Puta caso, devi lavorare con una quantità di dati abnorme; utilizzare la tabella precedente per analizzare è impensabile, hai una vaga idea di quanto grande verrebbe? Viene ad aiutarci il seguente ragionamento, scrivibile tramite \textbf{coefficienti binomiali}:
\begin{eg}
	\textbf{Calcolo combinatorio con coefficiente binomiale}\newline
	Diciamo di avere una gara a cui partecipano 10 atleti. In quanti modi possiamo assegnare i vari posti del podio? Avremo:
	\begin{itemize}
		\item 10 modi per il primo posto.
		\item 9 modi per il secondo, in quanto il primo è già stato assegnato.
		\item 8 modi per il terzo, per la medesima ragione.
	\end{itemize}
	Supponiamo di voler uccidere tutti quelli che perdono e che quindi considereremo solo i tre posti del podio. Allora quali sarebbero i possibili esiti?
	\begin{center}
		ABC, ACB, CAB, CBA, BAC, BCA, quindi $3\times 2\times 1 = 6$ esiti.
	\end{center}
	Questo calcolo si può esprimere più facilmente mediante l'utilizzo dei fattoriali, Gli esiti totali possibili saranno quindi:
	\begin{center}
		$\dfrac{(10\times9\times8)}{(3\times2\times1)} = \dfrac{10!}{7!\times3!}$
	\end{center}
	Perché è sbucato fuori un $7!$ dal nulla? Ebbene, quelli sono tutti i numeri che non ci interessano, in quanto vogliamo solamente i posti del podio.
\end{eg}
\noindent Da questo esempio traiamo dunque una formula generale, in inglese chiamata \textbf{n choose k}, la quale ha due varianti dipendentemente se ci interessa (caso 1) o meno (caso 2) l'ordine dei dati:
\begin{center}
	$\begin{cases}
		\dfrac{n!}{(n-k)!k!}\\
		\dfrac{n!}{(n-k)!}
	\end{cases} \implies \begin{pmatrix}
	n\\
	k
	\end{pmatrix}$
\end{center}

% TODO ----- Aggiungi esempio del mazzo da poker

%

\section{Probabilità condizionata}
Finora abbiamo utilizzato l'approccio frequentista per il calcolo delle probabilità di un evento, considerandole come a loro stanti. È tuttavia possibile che la probabilità di un evento $A$ possa essere influenzata da un altro $B$. Chiamiamo questo concetto \textbf{probabilità condizionata} e prende la formula matematica:
\begin{center}
	$P(A|B) = \dfrac{P(A\cap B)}{P(B)}$
\end{center}
Personalmente leggo la formula come "probabilità di $A$ sotto $B$". Quest'ultimo evento può quindi influenzare il primo positivamente, aumentandone la probabilità, oppure negativamente, diminuendola. Un'altra particolarità riguarda lo spazio campionario; essendo che stiamo valutando un'istanza dove $B$ avviene sicuramente, sarà proprio questo lo spazio. Possiamo infatti spaccare le istanze:
\begin{itemize}
	\item Succedono $A$ e $B$; quindi la probabilità condizionata.
	\item Succede $A$, ma non $B$; quindi la probabilità senza influenze.
\end{itemize}
È necessario conoscere i valori di entrambe le istanze per il calcolo della probabilità effettiva, infatti compone la seguente:
\begin{definition}
	\textbf{Formula delle probabilità totali}\par
	Formula utilizzata per il calcolo delle probabilità di un evento il cui esperire è condizionato da un altro. Siano $A,B$ due eventi generici.
	\begin{center}
		$P(A) = (A|B)P(B) + P(A|B^c)P(B^c)$
	\end{center}
	Dove il primo addendo rappresenta la probabilità condizionata ed il secondo quella di $A$ a sé stante.
\end{definition}

\begin{eg}
	\textbf{Calcolo di probabilità condizionata}\newline
	\noindent Siano due urne tali che:
	\begin{itemize}
		\item A contiene 2 palline rosse e 4 verdi.
		\item B contiene 3 palline rosse e 2 verdi.
	\end{itemize}
	Si lancia ora un dado; se esce 6 si estrae da A, altrimenti da B. Calcolare la probabilità di estrarre una pallina verde.\par
	Introduciamo i seguenti due eventi in base al risultato del dado:
	\begin{enumerate}
		\item $E$, Il dado mostra $6$.
		\item $F$, La pallina estratta è verde.
	\end{enumerate}
	\noindent Potremmo elencare ogni singola permutazione dati i pochi casi, ma useremo la formula della probabilità totale per pulizia. Attualmente deteniamo i seguenti dati:
	\begin{itemize}
		\item $P(F|E) = \frac{4}{6}$, date le $6$ palline di A, di cui $4$ verdi.
		\item $P(F|E^c) = \frac{2}{5}$, date le $5$ palline di B, di cui $2$ verdi.
		\item $P(E) = \frac{1}{6}$, probabilità del dado di far uscire $6$.
		\item $P(E^c) = \frac{5}{6}$, ogni altro numero del dado.
	\end{itemize}
	Ciò che abbiamo è sufficiente per utilizzare la formula della probabilità totale. Risulterà infatti:
	\begin{center}
		$P(F) = P(F|E)P(E) + P(F|E^c)P(E^c) = \dfrac{4}{6}\times \dfrac{1}{6} + \dfrac{2}{5}\times \dfrac{5}{6} = \dfrac{4}{9}$
	\end{center}
\end{eg}
\noindent Se pensi che sia possibile ottenere algebricamente le altre probabilità della formula, hai avuto un'ottima idea. Infatti per le probabilità condizionate abbiamo un nome apposito, che è:
\begin{definition}
	\textbf{Formula e teorema di Bayes}\par
	Necessaria per il calcolo della singola probabilità condizionata di un evento.
	\begin{center}
		$P(A|B) = \dfrac{P(B|A)P(A)}{P(B)}$
	\end{center}
	Sia ora ${B_1, B_2, ..., B_n}$ una partizione dello spazio campionario. Ne segue il teorema:
	\begin{center}
		$P(B_i|A) = \dfrac{P(A|B_i)P(B_i)}{\sum^n_{j=1} P(A|B_j)P(B_j)}$
	\end{center}
\end{definition}

\begin{eg}
	\textbf{Calcolo di probabilità con formula di Bayes}\par
	\noindent Abbiamo un esame a 4 risposte multiple. Gli studenti iscritti si dividono in:
	\begin{itemize}
		\item Preparato, corrispondente all'80\% del totale. Risponde correttamente al 90\%.
		\item Impreparato, il 20\% rimanente, che risponde a caso. Quindi hanno un 25\% di azzeccare la risposta.
	\end{itemize}
	Qual è la probabilità di prendere l'esame di uno studente preparato fra tutti?\newline
	
	\noindent Definiamo gli eventi come $A$, ovvero che lo studente sia preparato, e $B$, quella di azzeccare una risposta, che è necessariamente condizionata dal primo evento. Elenchiamo i dati che abbiamo fin da subito:
	\begin{itemize}
		\item $P(A) = 0,8$
		\item $P(A^c) = 0,2$
		\item $P(B|A) = 0,9$
		\item $P(B|A^c) = 0,25$
	\end{itemize}
	Dobbiamo trovare $P(A|B)$, ma procediamo per passi. Innanzitutto ci serve $P(B)$, ovvero la probabilità di azzeccare la risposta in generale. Usiamo la formula delle probabilità totali:
	\begin{center}
		$P(B) = P(B|A)P(A) + P(B|A^c)P(A^c) = 0,9\times 0,8 + 0,25\times 0,2 = 0,72 + 0,05 = 0,77$
	\end{center}
	Ora possiamo muoverci facendo delle asserzioni algebriche. Considera che $P(A|B)P(B) = P(A\cap B) = P(B\cap a) = P(B|A)P(A)$, da cui otteniamo la formula di Bayes: 
	\begin{center}
		$P(A|B) = \dfrac{P(B|A)P(A)}{P(B)}$
	\end{center}
	La quale, sostituendo le variabili con i loro rispettivi valori, ci darà il risultato:
	\begin{center}
		$P(A|B) = \dfrac{0,72}{0,77} = 0.935$
	\end{center}	
\end{eg}
E se invece avessimo due eventi completamente \textbf{indipendenti}? Questi si dicono tali se, dati per esempio A, B, vale la relazione:
\begin{center}
	$P(A \cap B) = P(A)P(B)$
\end{center}
Per esempio, se lancio due dadi, la probabilità che escano i valori 6 e 5 separatamente è $\frac{1}{36}$, perché mi va bene una sola combinazione. Infatti:
\begin{center}
	$P(A \cap B) = \dfrac{1}{6}\times \dfrac{1}{6} = \dfrac{1}{36}$
\end{center}
Abbiamo appurato che i due dadi non si influenzano fra di loro. Se ci fossero invece tre eventi avremmo le seguenti relazioni, dati A, B, C:
\begin{itemize}
	\item $P(A \cap B \cap C) = P(A)P(B)P(C)$
	\item $P(A \cap B) = P(A)P(B)$
	\item $P(A \cap C) = P(A)P(C)$
	\item $P(B \cap C) = P(B)P(C)$
\end{itemize}
Ovviamente, per casi richiedenti più di tre eventi, si dovranno verificare le istanze per tutti i successivi.

%

\section{Variabili aleatorie}
Le variabili aleatorie sono quantità numeriche il cui valore dipende dall'esito di un esperimento aleatorio; si indicano con $X, Y, Z$. La loro primaria utilità sta nel consentirci di poter considerare un risultato specifico al posto di ogni singolo evento.\par 
Per esempio, diciamo di lanciare due dadi e voler sapere la somma dei valori dei numeri che escono. Allora la variabile $X$ potrà assumere un valore dell'evento $x$ tale che:
\begin{center}
	$[(x\in S) \land (2 \leq x \leq 12)]$, dove $S$ è lo spazio campionario.
\end{center}
\noindent Possiamo inoltre ragionarci con la probabilità. Diciamo di voler effettuare un numero $n$ di lanci e che ci interessi vedere le chances che esca un dato numero. In questo caso $X$ sarà il valore totale dei successi ottenuti e la probabilità sarà data da $P(X = x)$.\newline

\noindent Di variabili aleatorie ne esistono due tipi:
\begin{itemize}
	\item \textbf{Discrete}; Se i valori che può assumere sono finiti o al più numerabili; quindi in un insieme $S = \{x_1, x_2, ..., x_n, ...\}$. Da questo spazio sappiamo che il calcolo della sua probabilità si effettua con la \textbf{funzione di massa}:
	\begin{center}
		$p(x) = P(X = x)$, dove grazie all'algebra vale $p(x_1), p(x_2), ..., p(x_n)$
	\end{center}
	Siccome stiamo considerando valori reali, è possibile che $X$ non possa assumere ogni numero. In questo caso il valore della probabilità dell'esito non possibile sarà, ovviamente, zero. Agli antipodi sta la probabilità massima, data dalla somma di tutte le chances di ogni esito. Sarà uguale ad uno, quindi $\sum_{x \in R} p(x) = 1$.
	\item \textbf{Continue}; Se esiste una funzione $f(x)$ detta \textbf{densità} della variabile aleatoria tale che per ogni insieme $A \subseteqq R$ si ha:
	\begin{center}
		\[P(X \in A) = \int_A f(x) dx\]
	\end{center}
	La ragione per cui è richiesto un integrale è che questo tipo di variabile aleatoria può assumere un range di valori reali. Essendo loro pressoché infiniti, rendendo anche la probabilità di esperirne uno solo infima, è necessario considerarli come una collettività. Altri casi sono:
	\begin{center}
		\[\int^b_a f(x)dx = P(a \leqq X \leqq b), \int^{+\infty}_0 f(x)dx = P(X \geqq 0), \int^{+\infty}_{-\infty} f(x) dx = 1\]
	\end{center}
\end{itemize}
\noindent Ora possiamo introdurre un nuovo concetto importante:
\begin{definition}
	\textbf{Valore atteso}\par
	\noindent Si tratta della media pesata dei possibili valori che $X$ può assumere e si scrive:
	\begin{itemize}
		\item Per variabili discrete: \[E(X) = \sum_{x \in R} xp(x) = x_1p(x_1) + x_2p(x_2) + ... + x_np(x_n)\]
		\item Per variabili continue: \[E(X) = \int^{+\infty}_{-\infty} xf(x) dx\]
	\end{itemize}
\end{definition}
\noindent Per eventuali diverse forme di X basta sostituire la forma alle $x_i$ piccole. Per esempio, sostituiremo $X^2$ come $x^2$ dove stanno tutte le $x$ nella formula.
\begin{eg}
	\textbf{Calcolo del valore atteso della variabile aleatoria discreta $X^2$}
	\[E(X^2) = \sum_{x \in R} x^2p(x) = x^2_1p(x_1) + x^2_2p(x_2) + ... + x^2_np(x_n)\]
\end{eg}

% TODO ----- INSERISCI ESEMPIO PRATICO DI CALCOLO

\noindent Questa formula viene con delle proprietà, le quali sono differenti per i due tipi di variabili in gioco:
\begin{itemize}
	\item $E(aX + b) = aE(X) + b$, dove $a,b \in \mathbb{R}$.
	\item Con $X,Y$ variabili aleatorie dipendenti da uno stesso esperimento: $E(X + Y) = E(X) + E(Y)$.
	\item Valore atteso di una funzione di variabile aleatoria: $E(g(X)) = \sum_{x \in R} g(x)p(x)$.
\end{itemize}
\noindent Per le variabili continue:
\begin{itemize}
	\item \[E(X) = \int_{-\infty}^{+\infty} xf(x) dx\]
	\item \[E(g(X)) = \int_{-\infty}^{+\infty} g(x)f(x) dx\]
\end{itemize}
\noindent In tal merito, sia una variabile aleatoria $X$ con il rispettivo valore atteso $\mu$, quindi $E(X) = \mu$. Da qui possiamo ottenere il concetto di \textbf{varianza}, ovvero il valore atteso degli scarti.
\begin{center}
	$Var(X) = E[(X-\mu)^2] = E(X^2) - \mu^2$ (grazie all'algebra)
\end{center}
\noindent Per calcolare questo valore abbiamo bisogno di due elementi: il valore atteso $E(X)$ e al quadrato $E(X^2)$. I calcoli sono diversi in base al tipo di variabile preso in esame:
\begin{enumerate}
	\item Valore atteso grado 1:\par 
	\[\mu = E(X) = \begin{cases}
		\sum_{x \in R} xp(x)\\
		\int^{+\infty}_{-\infty} xf(x)dx
	\end{cases}\]
	\item Valore atteso al quadrato:
	\[E(X^2) = \begin{cases}
		\sum_{x^2 \in R} x^2p(x)\\
		\int^{+\infty}_{-\infty} x^2f(x)dx
	\end{cases}\]
\end{enumerate}
\noindent Notare infine come ultima cosa che la varianza è un valore compreso fra $0$ e $1$.

% TODO ----- AGGIUNGI ESEMPIO CON ESERCIZIO

%

\section{Distribuzioni congiunte}
Può capitare di lavorare con variabili dipendenti da uno stesso esperimento. In tal caso avremo una funzione di massa \textbf{congiunta} scritta come:
\begin{center}
	$p_{X,Y}(x,y) = P(X = x, Y = y)$, con $X,Y$ variabili aleatorie discrete.
\end{center}
\noindent Per esempio, lanciamo un dado e definiamo le due variabili aleatorie discrete:
\begin{itemize}
	\item X = punteggio più basso.
	\item Y = punteggio più alto.
\end{itemize}
\noindent Ne deriva necessariamente che, volendo sapere la probabilità che esca un certo numero:
\begin{itemize}
	\item $P_{X,Y}(1,1) = P(X = 1, Y = 1) = \frac{1}{36}$, perché in ambo i lanci è uscito 1.
	\item $P_{X,Y}(1,2) = P(X = 1, Y = 2) = \frac{1}{18}$, perché stai sommando le probabilità che escano i numeri nell'ordine $(1,2), (2,1)$. Ciò risulta ovviamente più probabile piuttosto che due numeri singoli.
\end{itemize}
\noindent Un'altra cosa importante è che ci è possibile ottenere la funzione di massa di una singola variabile a partire da quella congiunta\footnote{Non è possibile il contrario, tuttavia. Soffri.}, infatti:
\begin{itemize}
	\item $P(X = x) = p(x) = \sum_{y \in \mathbb{R}} p_{X,Y}(x,y)$
	\item $P(Y = y) = p(y) = \sum_{x \in \mathbb{R}} p_{X,Y}(x,y)$
\end{itemize}
\noindent In questo contesto, le funzioni di massa $p_X, p_Y$ sono dette \textbf{marginali}. Come ben penserai, è possibile calcolare il valore atteso di una funzione di due variabili aleatorie nel seguente modo:
\begin{center}
	\[E[g(X,Y)] = \sum_{x,y \in R} g(x,y)p_{X,Y}(x,y)\]
\end{center}
\begin{definition}
	\textbf{Variabili aleatorie indipendenti}\par
	\noindent Due variabili aleatorie si dicono tali se valgono le seguenti relazioni:
	\begin{enumerate}
		\item $P(X \in A, Y \in B) = P(X \in A)P(Y \in B)$.
		\item $P(X \in A | Y \in B) = P(X \in A)$, equivalente alla prima.
		\item $P(X \in B | Y \in A) = P(X \in B)$, equivalente alla prima.
		\item $P_{X,Y}(x,y) = p_X(x)p_Y(y)$.
	\end{enumerate}
\end{definition}

% TODO ----- IL PROF DEVE SPIEGARE COVARIANZA

%

\section{Classi notevoli di variabili aleatorie}

%

\section{Statistiche campionarie}