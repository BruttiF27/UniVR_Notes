\section{Architettura LC-3}
Il microprocessore \textbf{Little Computer 3} è un'architettura CPU basata sul modello di Von Neumann della quale studieremo struttura, caratteristiche e ISA.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{Images/LC3.png}
    \caption{Modello di architettura LC-3}
    \label{fig:enter-label}
\end{figure}
\textbf{- Struttura}
\textbf{- Divisione in bit delle istruzioni e stati di processo}
\textbf{ISA LC-3}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Modello CISC e RISC}
My dear Reader, che ne pensi di una sezione relax dove ti racconto una storia al posto delle solite spiegazioni? Davvero, zero immagini, zero presura e mettiamoci comodi.\par\quad
Parliamo di com'è nato il modello di architettura sul quale stai con ogni probabilità leggendo questo PDF: il \textbf{RISC}\footnote{Reduced Instruction Set Computer - Elaboratore dall'insieme di istruzioni ridotto.}; figlio del connubio di tutto ciò che abbiamo studiato finora. Si tratta dell'architettura con l'attuale migliore ottimizzazione del \textit{tempo impiegato dalla CPU}\footnote{Più precisamente, vale l'equazione: $T_{CPU} = Tot_{Istruzioni} \times CPI_{Medio} \times \frac{1}{Freq_{Clock}}$.} per elaborare le informazioni.\newline

Ma partiamo dal principio: Giusto neanche cinquant'anni fa, il mondo ha visto una grandissima opportunità di mercato nei microprocessori, e di conseguenza molte aziende, come Intel, hanno provato a prendere parte alla corsa per fare i grossi soldi.\par\quad
L'intenzione è creare il microprocessore più efficiente possibile per sbaragliare la concorrenza. Come fare? La prima idea diffusa su vasta scala fu quella di \textit{aumentare la frequenza dei cicli di clock}. Essendo che la frequenza del clock è data dal Datapath, il quale ha un cammino critico, si pensò di utilizzare delle pipelines per ottimizzare le istruzioni dell'ISA.\par\quad
Fatto sta che il funzionamento della pipeline dipende dalla quantità ed il tipo di istruzioni presenti nell'insieme e siamo inoltre limitati dalla potenza delle componenti fisiche. Agiremo, ordunque, sul valore totale delle istruzioni nell'ISA, modificandole e rendendole molto più complesse e compatte. Abbiamo ottenuto un valore molto minore di direttive, ma avremo come conseguenza un datapath che rispecchia la complessità di quanto rielaborato e quindi un cammino critico devastante, per non parlare della pipeline che deve costantemente aspettare il lavoro finito dello stato precedente. Doesn't sound that efficient now, does it?\newline

Qui entrarono in gioco due professori dell'Università di Stanford; coloro che ebbero il coraggio e l'astuzia di pensare fuori dal pensiero comune, creando l'architettura sulla quale si basano gli elaboratori contemporanei: \textit{John Hennessy} ed il suo collaboratore \textit{David Patterson}.\par\quad
"E se fossero le istruzioni a dover essere ridotte per ottenere prestazioni migliori?" Fu la fatidica domanda che smosse il tutto. Chiesero ad altri ricercatori di creare dei programmi in diversi linguaggi e notarono una particolarità fra tutti i codici: erano scritti in modo \textit{semplice} indipendentemente dalla complessità del linguaggio. Come diretta conseguenza, anche le istruzioni in Assembly erano semplici.\par\quad
Con questa filosofia i due riuscirono a creare il DLX, il primo vero dispositivo RISC e con i dati alla mano, nonostante i soliti dubbi dal mercato, riuscirono a dimostrare di aver creato un microprocessore nettamente migliore a quelli delle altre aziende, che decisero di chiamare \textbf{CISC}\footnote{Complex Instruction Set Computer - Elaboratore dall'insieme di istruzioni complesso.}. I dispositivi RISC, oltre ad avere poche istruzioni, hanno il vantaggio di avere pochi metodi di indirizzamento. Questa semplicità aiuta il funzionamento della pipeline, la quale sarà nettamente più veloce, ottenendo non solo un numero molto minore di CPI medio, ma anche una frequenza di clock maggiore rispetto alle altre architetture.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{Images/turing_hennessy_patterson_book.jpg}
    \caption{John Hennessy e David Patterson}
    \label{fig:enter-label}
\end{figure}

Annusata l'opportunità per fare soldi, le aziende iniziarono a cimentarsi nella filosofia RISC, ottenendo risultati sulla potenza in pochissimo tempo:
\begin{itemize}
    \item \textbf{Intel} fu come al solito la prima e creò il \textit{MIPS}, con una frequenza di 100MHz.
    \item \textbf{Motorola} seguì subito con la creazione di \textit{PowerPC}, da 200MHz.
    \item \textbf{DEC} poi creò \textit{Alpha} dai sorprendenti 600MHz.
    \item Il \textbf{Stanford University Network} infine creò la \textit{SPARC} su FPGA, da 300MHz.
\end{itemize}

Nella foga per l'ottimizzazione, gli ingegneri a Intel crearono il microprocessore P6, che per quanto ottimale potesse risultare al tempo, aveva un ISA completamente diverso da quello utilizzato da loro finora (e tuttora); Intel 80x86.\par\quad
Si dovette creare un chip più grande con lo scopo di, mediante una fase di \textit{Pre-Fetch}, tradurre l'ISA diverso nell'80x86. Chiamarono questa architettura \textit{Pentium}.\par\quad
Attualmente nessuno di questi microprocessori è più in uso, ma immagino tu già lo sappia, dato che abbiamo ben superato il GigaHz di frequenza. 

%

\section{Architetture parallele}
Allora, è nostro volere ottimizzare \textit{even further beyond}, quindi bisogna capire cosa vuol dire veramente migliorare le prestazioni di un calcolatore.\par\quad
Da un punto di vista dell'utente medio si può pensare a ridurre il tempo totale di esecuzione della CPU, che non è necessariamente sbagliato, ma è un ragionamento superficiale. Si ritiene invece più importante aumentare il numero di processi eseguibili in un determinato arco di tempo ed è questo ragionamento che ha portato allo sviluppo delle \textbf{Architetture Superscalari}, modelli con la capacità di elaborare istruzioni con un CPI medio minore di 1, ma come raggiungere questo risultato?\par\quad
Il primo microprocessore vincente fu il \textit{PowerPC} di Motorola, il quale utilizzava una componente di pre-fetch che inviava poi i dati da elaborare a \textit{due pipelines} da quattro stati l'una. La prima avrebbe svolto operazioni solamente con interi, l'altra con numeri in virgola mobile.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{Images/PowerPC Pipelines.png}
    \caption{Funzionamento dual-pipeline di PowerPC}
    \label{fig:enter-label}
\end{figure}

Queste due pipelines funzionano \textit{in parallelo}, quindi dove normalmente avremmo un CPI = 2, data la "doppia velocità", abbiamo il CPI = 0,5. Inoltre abbiamo la possibilità di donare loro un clock proprio per gestire al meglio le operazioni ed aumentare conseguentemente ancora di più la frequenza di clock. Se far ciò migliora così tanto le prestazioni, perché non inserire sempre più pipelines che lavorano parallelamente? Ti invito a richiamare alla memoria la \textit{dipendenza delle istruzioni}. Ulteriori pipelines equivalgono ad ulteriori condizioni e più ce ne sono, più (se necessario, ma di norma lo è) bisognerà attendere la fine della fase precedente, rallentando l'intero processo e peggiorando le prestazioni. Che fare, ordunque?\par\quad
Un primo tentativo di risoluzione si ebbe con le architetture di microprocessori \textbf{VLIW}\footnote{Very Long Instruction Word - Parola d'istruzione molto lunga.}, il cui scopo è ottenere un livello di astrazione maggiore per le istruzioni, le quali saranno quindi composte di più operazioni, aumentando la dimensione dei banchi di memoria presenti fra la pipeline.\par\quad
Di una diversa linea di pensiero furono i creatori dei \textbf{Calcolatori Vettoriali}; i quali pensarono di utilizzare, insieme alla componente di pre-Fetch e ad un banco di memoria, molte più CPU che lavorano in parallelo. Per esempio, se bisogna eseguire 1000 somme normalmente utilizzeresti 1000 cicli di clock, ma se inseriamo 500 circuiti di somma, ne utilizzeremo solamente 2; un miglioramento spaventoso. Tuttavia, per collegare le mini-CPU era necessario utilizzare una \textit{matrice di interconnessione}, che causava una sovrabbondanza di collegamenti.\par\quad
Un altro problema è presentato dalla \textbf{Legge di Amdahl}, la quale afferma: "\textit{Il miglioramento delle prestazioni di un sistema che si può ottenere ottimizzando una certa parte del sistema è limitato dalla frazione di tempo in cui tale parte è effettivamente utilizzata}".\par
In merito introduco il concetto di \textbf{Speed-up}, che ha origine proprio grazie a questa legge. Si tratta in genere di un valore che misura le prestazioni di un elaboratore in funzione di un determinato programma; in particolare ci interessa la \textit{Latenza}\footnote{$L = \frac{T}{W}$, dove T è il tempo e W il totale del lavoro eseguito per la task.} per poter eventualmente paragonare i tempi di due architetture effettuando un rapporto fra di loro.\par
Visto questo, immagina un calcolatore vettoriale che cerca di svolgere qualsiasi compito diverso da operazioni aritmetiche. Non sembra molto efficiente.\newline

Ipotizziamo ora di non volere un parallelismo per ogni singolo programma, bensì di lavorare su un parallelismo fra i processi e quindi avere più CPU in una singola architettura. Il problema fondamentale di questa idea è che ognuna di loro necessita la propria cache e nell'elaborazione dei dati avremo delle copie esatte di quanto elaborato in ogni CPU, quindi un problema di \textit{Coerenza di cache}.\par\quad
Per la gestione di questo problema vennero ideati gli \textbf{Algoritmi di Snooping}, che vedono la presenza in ogni microprocessore di una tabella contenente le posizioni di $n$ dati, dopo che questi sono passati per il BUS. Non sono tuttavia soluzioni perfette, poiché il troppo snooping peggiora le prestazioni, senza contare che abbiamo un singolo BUS dove passano le informazioni\footnote{Se non ti sembra aver senso invito la revisione della sezione dedicata all'arbitraggio del BUS.}.\newline

Poi venne il nuovo millennio e cambiò tutto con la tecnologia \textbf{Network On Chip}; la realizzazione di una matrice di interconnessione sul silicio stesso, che consentì di inserire più CPU in una singola architettura. Questi processori vengono chiamati \textit{Cores} e questo miracolo diede vita ai processori \textbf{Multicore}, inseriti nei dispositivi come quello che stai utilizzando ora. Il grande vantaggio è avere una cache consistente poiché le connessioni della matrice annichiliscono le attese per accedervi.\par\quad
Ma non è finita qui; venne data nuova vita ai calcolatori vettoriali per la necessità della rapida elaborazione delle grafiche a video. Si tratta di un compito particolarmente complesso basato su calcoli computazionali; il punto di forza di tali architetture. Oggi sono conosciuti sotto il nome di GPU\footnote{Graphic Processing Unit - Unità di elaborazione grafica.} ed il capo supremo del loro mercato è NVidia.\par\quad
Col passare del tempo questi modelli si svilupparono e videro la luce le GPGPU\footnote{General Purpose GPU}, utilizzate per richieste grafiche particolarmente esigenti, come un qualunque gioco PS5 realistico. Si presta inoltre molto bene per la creazione di reti neurali, quindi per la creazione e lo sviluppo di intelligenze artificiali.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\linewidth]{Images/ModernArchitecture.png}
    \caption{Architettura di Von Neumann contemporanea}
    \label{fig:enter-label}
\end{figure}

\newpage
